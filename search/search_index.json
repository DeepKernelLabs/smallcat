{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"About","text":"smallcat <p>A small, modular data catalog.</p> <p> </p>"},{"location":"#install","title":"Install","text":"<pre><code>pip install smallcat\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":""},{"location":"#create-catalog","title":"Create Catalog","text":"<p>Local catalogs can be kept in YAML files.</p> <pre><code>entries:\n    foo:\n        file_format: csv\n        connection:\n            conn_type: fs\n            extra:\n                base_path: /tmp/smallcat-example/\n        location: foo.csv\n        load_options:\n            header: true\n    bar:\n        file_format: parquet\n        connection:\n            conn_type: google_cloud_platform\n            extra:\n                bucket: my-bucket\n        location: bar.csv\n        save_options:\n            partition_by:\n                - year\n                - month\n</code></pre>"},{"location":"#standalone","title":"Standalone","text":"<pre><code>from smallcat import Catalog\n\ncatalog = Catalog.from_path(\"catalog.yaml\")\ncatalog.save_pandas(\"foo\", df)\ndf2 = catalog.load_pandas(\"foo\")\n</code></pre>"},{"location":"#filter-on-load","title":"Filter on load","text":"<p><code>load_pandas</code> (and the lower-level Arrow loaders) accept optional <code>where</code> and <code>columns</code> arguments to push filters and projections down to DuckDB/Arrow when reading:</p> <pre><code>df = catalog.load_pandas(\n    \"bar\",\n    where=\"event_date &gt;= '2024-01-01'\",\n    columns=[\"event_date\", \"user_id\"],\n)\n</code></pre>"},{"location":"#with-airflow","title":"With Airflow","text":"<pre><code>from smallcat import Catalog\n\ncatalog = Catalog.from_airflow_variable(\"example_catalog\")\ndf = catalog.load_pandas(\"bar\")\n</code></pre>"},{"location":"api/catalog/","title":"Catalog","text":""},{"location":"api/catalog/#smallcat.catalog.Catalog","title":"smallcat.catalog.Catalog","text":"<p>               Bases: <code>BaseModel</code></p> <p>A collection of named datasets with associated loader configuration.</p> <p>The catalog maps user-defined keys to concrete dataset entries (e.g., CSV or Excel). It can be constructed from an in-memory dictionary, an Airflow Variable (JSON), or a YAML file.</p> <p>Attributes:</p> Name Type Description <code>entries</code> <code>dict[str, CatalogEntry]</code> <p>Mapping of dataset names to their configurations.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>class Catalog(BaseModel):\n    \"\"\"A collection of named datasets with associated loader configuration.\n\n    The catalog maps user-defined keys to concrete dataset entries (e.g., CSV or\n    Excel). It can be constructed from an in-memory dictionary, an Airflow\n    Variable (JSON), or a YAML file.\n\n    Attributes:\n        entries: Mapping of dataset names to their configurations.\n    \"\"\"\n\n    entries: dict[str, CatalogEntry] = Field(\n        ...,\n        description=\"Named data sets\",\n    )\n\n    @staticmethod\n    def from_dict(dictionary: dict) -&gt; \"Catalog\":\n        \"\"\"Create a catalog from a Python dictionary.\n\n        The dictionary must conform to the `Catalog` schema (i.e., include an\n        `entries` key mapping names to valid `CatalogEntry` objects).\n\n        Args:\n            dictionary: A dictionary matching the `Catalog` model.\n\n        Returns:\n            Catalog: A validated `Catalog` instance.\n\n        Raises:\n            pydantic.ValidationError: If the dictionary does not match the schema.\n        \"\"\"\n        return Catalog.model_validate(dictionary)\n\n    @staticmethod\n    def from_airflow_variable(variable_id: str) -&gt; \"Catalog\":\n        \"\"\"Create a catalog from an Airflow Variable containing JSON.\n\n        The Airflow Variable should contain a JSON object compatible with the\n        `Catalog` schema.\n\n        Args:\n            variable_id: The Airflow Variable ID to read (expects JSON).\n\n        Returns:\n            Catalog: A `Catalog` instance constructed from the Variable value.\n\n        Raises:\n            KeyError: If the Airflow Variable does not exist.\n            pydantic.ValidationError: If the JSON payload is invalid for the model.\n        \"\"\"\n        try:\n            from airflow.sdk import Variable\n        except ImportError:\n            from airflow.models import Variable  # type: ignore[attr-defined,no-redef] # noqa: I001\n\n        try:\n            dictionary_entries = Variable.get(variable_id, deserialize_json=True)\n        except TypeError:\n            # LocalFilesystemBackend can return an object causing a TypeError\n            # In this case we don't need to deserialize into JSON\n            #  as it's not a string\n            dictionary_entries = Variable.get(variable_id)\n        except ImportError as e:\n            # Airflow fails with import error if variable is not present and tries\n            #  to talk to the Task Supervisor (the runner process) over an internal\n            #  comms channel (SUPERVISOR_COMMS) to fetch it.\n            msg = f\"Variable {variable_id} not found in Airflow\"\n            raise KeyError(msg) from e\n        return Catalog.from_dict(dictionary_entries)\n\n    @staticmethod\n    def from_yaml(dictionary_path: str | Path) -&gt; \"Catalog\":\n        \"\"\"Create a catalog from a YAML file.\n\n        Args:\n            dictionary_path: Path to a YAML file whose content matches the\n                `Catalog` schema.\n\n        Returns:\n            Catalog: A `Catalog` instance constructed from the YAML content.\n\n        Raises:\n            FileNotFoundError: If the YAML file cannot be found.\n            pydantic.ValidationError: If the YAML content is invalid for the model.\n        \"\"\"\n        with Path(dictionary_path).open() as f:\n            catalog_dict = yaml.safe_load(f)\n        return Catalog.from_dict(catalog_dict)\n\n    def _get_entry(self, key: str) -&gt; EntryBase:\n        try:\n            return self.entries[key]\n        except KeyError as e:\n            msg = f\"Entry {key} not found in dictionary\"\n            raise KeyError(msg) from e\n\n    def get_dataset(self, key: str) -&gt; BaseDataset:\n        \"\"\"Instantiate a concrete dataset for a given catalog entry.\n\n        Args:\n            key: The name of the catalog entry to resolve.\n\n        Returns:\n            BaseDataset: A dataset instance ready to load/save the data.\n\n        Raises:\n            KeyError: If the key is not present in the catalog.\n            ValueError: If the entry's `file_format` is not supported.\n        \"\"\"\n        entry = self._get_entry(key)\n        return entry.build_dataset()\n\n    def load_pandas(\n        self,\n        key: str,\n        where: str | None = None,\n        columns: list[str] | None = None,\n    ) -&gt; \"pd.DataFrame\":\n        \"\"\"Load a dataset from the catalog into a pandas DataFrame.\n\n        Resolves the catalog entry identified by ``key`` and delegates to\n        :meth:`EntryBase.load_pandas`. This is equivalent to:\n\n            ``self.entries[key].build_dataset().load_pandas(entry.location)``\n\n        Args:\n            key: The catalog entry name to load.\n            where: Optional SQL filter predicate forwarded to the dataset.\n            columns: Optional list of columns to project.\n\n        Returns:\n            pd.DataFrame: The loaded tabular data.\n\n        Raises:\n            KeyError: If ``key`` is not present in the catalog.\n            Exception: Any error propagated from the underlying dataset's loader.\n        \"\"\"\n        entry = self._get_entry(key)\n        return entry.load_pandas(where=where, columns=columns)\n\n    def save_pandas(self, key: str, df: \"pd.DataFrame\") -&gt; None:\n        \"\"\"Save a pandas DataFrame to a dataset in the catalog.\n\n        Resolves the catalog entry identified by ``key`` and delegates to\n        :meth:`EntryBase.save_pandas`. This writes to the entry's configured\n        ``location`` with any format-specific save options applied.\n\n        Args:\n            key: The catalog entry name to write to.\n            df (pd.DataFrame): The DataFrame to persist.\n\n        Raises:\n            KeyError: If ``key`` is not present in the catalog.\n            Exception: Any error propagated from the underlying dataset's saver.\n        \"\"\"\n        entry = self._get_entry(key)\n        entry.save_pandas(df)\n\n    def load_arrow(\n        self,\n        key: str,\n        where: str | None = None,\n        columns: list[str] | None = None,\n    ) -&gt; \"pa.Table\":\n        \"\"\"Load a dataset from the catalog into an Apache Arrow Table.\n\n        Resolves the catalog entry identified by `key` and delegates to\n        :meth:`EntryBase.load_arrow`. This is equivalent to:\n\n            `self.entries[key].build_dataset().load_arrow_table(entry.location)`\n\n        Args:\n            key: The catalog entry name to load.\n            where: Optional SQL filter predicate forwarded to the dataset.\n            columns: Optional list of columns to project.\n\n        Returns:\n            pa.Table: The loaded Arrow table.\n\n        Raises:\n            KeyError: If `key` is not present in the catalog.\n            Exception: Any error propagated from the underlying dataset's loader.\n        \"\"\"\n        entry = self._get_entry(key)\n        return entry.load_arrow(where=where, columns=columns)\n\n    def save_arrow(self, key: str, table: \"pa.Table\") -&gt; None:\n        \"\"\"Save an Apache Arrow Table to a dataset in the catalog.\n\n        Resolves the catalog entry identified by `key` and delegates to\n        :meth:`EntryBase.save_arrow`. This writes to the entry's configured\n        `location` with any format-specific save options applied.\n\n        Args:\n            key: The catalog entry name to write to.\n            table (pa.Table): The Arrow table to persist.\n\n        Raises:\n            KeyError: If `key` is not present in the catalog.\n            Exception: Any error propagated from the underlying dataset's saver.\n        \"\"\"\n        entry = self._get_entry(key)\n        entry.save_arrow(table)\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.Catalog.entries","title":"entries  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>entries: dict[str, CatalogEntry] = Field(..., description='Named data sets')\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.Catalog.from_dict","title":"from_dict  <code>staticmethod</code>","text":"<pre><code>from_dict(dictionary: dict) -&gt; Catalog\n</code></pre> <p>Create a catalog from a Python dictionary.</p> <p>The dictionary must conform to the <code>Catalog</code> schema (i.e., include an <code>entries</code> key mapping names to valid <code>CatalogEntry</code> objects).</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>A dictionary matching the <code>Catalog</code> model.</p> required <p>Returns:</p> Name Type Description <code>Catalog</code> <code>Catalog</code> <p>A validated <code>Catalog</code> instance.</p> <p>Raises:</p> Type Description <code>pydantic.ValidationError</code> <p>If the dictionary does not match the schema.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>@staticmethod\ndef from_dict(dictionary: dict) -&gt; \"Catalog\":\n    \"\"\"Create a catalog from a Python dictionary.\n\n    The dictionary must conform to the `Catalog` schema (i.e., include an\n    `entries` key mapping names to valid `CatalogEntry` objects).\n\n    Args:\n        dictionary: A dictionary matching the `Catalog` model.\n\n    Returns:\n        Catalog: A validated `Catalog` instance.\n\n    Raises:\n        pydantic.ValidationError: If the dictionary does not match the schema.\n    \"\"\"\n    return Catalog.model_validate(dictionary)\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.Catalog.from_airflow_variable","title":"from_airflow_variable  <code>staticmethod</code>","text":"<pre><code>from_airflow_variable(variable_id: str) -&gt; Catalog\n</code></pre> <p>Create a catalog from an Airflow Variable containing JSON.</p> <p>The Airflow Variable should contain a JSON object compatible with the <code>Catalog</code> schema.</p> <p>Parameters:</p> Name Type Description Default <code>variable_id</code> <code>str</code> <p>The Airflow Variable ID to read (expects JSON).</p> required <p>Returns:</p> Name Type Description <code>Catalog</code> <code>Catalog</code> <p>A <code>Catalog</code> instance constructed from the Variable value.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the Airflow Variable does not exist.</p> <code>pydantic.ValidationError</code> <p>If the JSON payload is invalid for the model.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>@staticmethod\ndef from_airflow_variable(variable_id: str) -&gt; \"Catalog\":\n    \"\"\"Create a catalog from an Airflow Variable containing JSON.\n\n    The Airflow Variable should contain a JSON object compatible with the\n    `Catalog` schema.\n\n    Args:\n        variable_id: The Airflow Variable ID to read (expects JSON).\n\n    Returns:\n        Catalog: A `Catalog` instance constructed from the Variable value.\n\n    Raises:\n        KeyError: If the Airflow Variable does not exist.\n        pydantic.ValidationError: If the JSON payload is invalid for the model.\n    \"\"\"\n    try:\n        from airflow.sdk import Variable\n    except ImportError:\n        from airflow.models import Variable  # type: ignore[attr-defined,no-redef] # noqa: I001\n\n    try:\n        dictionary_entries = Variable.get(variable_id, deserialize_json=True)\n    except TypeError:\n        # LocalFilesystemBackend can return an object causing a TypeError\n        # In this case we don't need to deserialize into JSON\n        #  as it's not a string\n        dictionary_entries = Variable.get(variable_id)\n    except ImportError as e:\n        # Airflow fails with import error if variable is not present and tries\n        #  to talk to the Task Supervisor (the runner process) over an internal\n        #  comms channel (SUPERVISOR_COMMS) to fetch it.\n        msg = f\"Variable {variable_id} not found in Airflow\"\n        raise KeyError(msg) from e\n    return Catalog.from_dict(dictionary_entries)\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.Catalog.from_yaml","title":"from_yaml  <code>staticmethod</code>","text":"<pre><code>from_yaml(dictionary_path: str | Path) -&gt; Catalog\n</code></pre> <p>Create a catalog from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>dictionary_path</code> <code>str | Path</code> <p>Path to a YAML file whose content matches the <code>Catalog</code> schema.</p> required <p>Returns:</p> Name Type Description <code>Catalog</code> <code>Catalog</code> <p>A <code>Catalog</code> instance constructed from the YAML content.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the YAML file cannot be found.</p> <code>pydantic.ValidationError</code> <p>If the YAML content is invalid for the model.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>@staticmethod\ndef from_yaml(dictionary_path: str | Path) -&gt; \"Catalog\":\n    \"\"\"Create a catalog from a YAML file.\n\n    Args:\n        dictionary_path: Path to a YAML file whose content matches the\n            `Catalog` schema.\n\n    Returns:\n        Catalog: A `Catalog` instance constructed from the YAML content.\n\n    Raises:\n        FileNotFoundError: If the YAML file cannot be found.\n        pydantic.ValidationError: If the YAML content is invalid for the model.\n    \"\"\"\n    with Path(dictionary_path).open() as f:\n        catalog_dict = yaml.safe_load(f)\n    return Catalog.from_dict(catalog_dict)\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.Catalog._get_entry","title":"_get_entry","text":"<pre><code>_get_entry(key: str) -&gt; EntryBase\n</code></pre> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def _get_entry(self, key: str) -&gt; EntryBase:\n    try:\n        return self.entries[key]\n    except KeyError as e:\n        msg = f\"Entry {key} not found in dictionary\"\n        raise KeyError(msg) from e\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.Catalog.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(key: str) -&gt; BaseDataset\n</code></pre> <p>Instantiate a concrete dataset for a given catalog entry.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The name of the catalog entry to resolve.</p> required <p>Returns:</p> Name Type Description <code>BaseDataset</code> <code>BaseDataset</code> <p>A dataset instance ready to load/save the data.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the key is not present in the catalog.</p> <code>ValueError</code> <p>If the entry's <code>file_format</code> is not supported.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def get_dataset(self, key: str) -&gt; BaseDataset:\n    \"\"\"Instantiate a concrete dataset for a given catalog entry.\n\n    Args:\n        key: The name of the catalog entry to resolve.\n\n    Returns:\n        BaseDataset: A dataset instance ready to load/save the data.\n\n    Raises:\n        KeyError: If the key is not present in the catalog.\n        ValueError: If the entry's `file_format` is not supported.\n    \"\"\"\n    entry = self._get_entry(key)\n    return entry.build_dataset()\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.Catalog.load_pandas","title":"load_pandas","text":"<pre><code>load_pandas(key: str, where: str | None = None, columns: list[str] | None = None) -&gt; pd.DataFrame\n</code></pre> <p>Load a dataset from the catalog into a pandas DataFrame.</p> <p>Resolves the catalog entry identified by <code>key</code> and delegates to :meth:<code>EntryBase.load_pandas</code>. This is equivalent to:</p> <pre><code>``self.entries[key].build_dataset().load_pandas(entry.location)``\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The catalog entry name to load.</p> required <code>where</code> <code>str | None</code> <p>Optional SQL filter predicate forwarded to the dataset.</p> <code>None</code> <code>columns</code> <code>list[str] | None</code> <p>Optional list of columns to project.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The loaded tabular data.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If <code>key</code> is not present in the catalog.</p> <code>Exception</code> <p>Any error propagated from the underlying dataset's loader.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def load_pandas(\n    self,\n    key: str,\n    where: str | None = None,\n    columns: list[str] | None = None,\n) -&gt; \"pd.DataFrame\":\n    \"\"\"Load a dataset from the catalog into a pandas DataFrame.\n\n    Resolves the catalog entry identified by ``key`` and delegates to\n    :meth:`EntryBase.load_pandas`. This is equivalent to:\n\n        ``self.entries[key].build_dataset().load_pandas(entry.location)``\n\n    Args:\n        key: The catalog entry name to load.\n        where: Optional SQL filter predicate forwarded to the dataset.\n        columns: Optional list of columns to project.\n\n    Returns:\n        pd.DataFrame: The loaded tabular data.\n\n    Raises:\n        KeyError: If ``key`` is not present in the catalog.\n        Exception: Any error propagated from the underlying dataset's loader.\n    \"\"\"\n    entry = self._get_entry(key)\n    return entry.load_pandas(where=where, columns=columns)\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.Catalog.save_pandas","title":"save_pandas","text":"<pre><code>save_pandas(key: str, df: pd.DataFrame) -&gt; None\n</code></pre> <p>Save a pandas DataFrame to a dataset in the catalog.</p> <p>Resolves the catalog entry identified by <code>key</code> and delegates to :meth:<code>EntryBase.save_pandas</code>. This writes to the entry's configured <code>location</code> with any format-specific save options applied.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The catalog entry name to write to.</p> required <code>df</code> <code>pd.DataFrame</code> <p>The DataFrame to persist.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If <code>key</code> is not present in the catalog.</p> <code>Exception</code> <p>Any error propagated from the underlying dataset's saver.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def save_pandas(self, key: str, df: \"pd.DataFrame\") -&gt; None:\n    \"\"\"Save a pandas DataFrame to a dataset in the catalog.\n\n    Resolves the catalog entry identified by ``key`` and delegates to\n    :meth:`EntryBase.save_pandas`. This writes to the entry's configured\n    ``location`` with any format-specific save options applied.\n\n    Args:\n        key: The catalog entry name to write to.\n        df (pd.DataFrame): The DataFrame to persist.\n\n    Raises:\n        KeyError: If ``key`` is not present in the catalog.\n        Exception: Any error propagated from the underlying dataset's saver.\n    \"\"\"\n    entry = self._get_entry(key)\n    entry.save_pandas(df)\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.Catalog.load_arrow","title":"load_arrow","text":"<pre><code>load_arrow(key: str, where: str | None = None, columns: list[str] | None = None) -&gt; pa.Table\n</code></pre> <p>Load a dataset from the catalog into an Apache Arrow Table.</p> <p>Resolves the catalog entry identified by <code>key</code> and delegates to :meth:<code>EntryBase.load_arrow</code>. This is equivalent to:</p> <pre><code>`self.entries[key].build_dataset().load_arrow_table(entry.location)`\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The catalog entry name to load.</p> required <code>where</code> <code>str | None</code> <p>Optional SQL filter predicate forwarded to the dataset.</p> <code>None</code> <code>columns</code> <code>list[str] | None</code> <p>Optional list of columns to project.</p> <code>None</code> <p>Returns:</p> Type Description <code>pa.Table</code> <p>pa.Table: The loaded Arrow table.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If <code>key</code> is not present in the catalog.</p> <code>Exception</code> <p>Any error propagated from the underlying dataset's loader.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def load_arrow(\n    self,\n    key: str,\n    where: str | None = None,\n    columns: list[str] | None = None,\n) -&gt; \"pa.Table\":\n    \"\"\"Load a dataset from the catalog into an Apache Arrow Table.\n\n    Resolves the catalog entry identified by `key` and delegates to\n    :meth:`EntryBase.load_arrow`. This is equivalent to:\n\n        `self.entries[key].build_dataset().load_arrow_table(entry.location)`\n\n    Args:\n        key: The catalog entry name to load.\n        where: Optional SQL filter predicate forwarded to the dataset.\n        columns: Optional list of columns to project.\n\n    Returns:\n        pa.Table: The loaded Arrow table.\n\n    Raises:\n        KeyError: If `key` is not present in the catalog.\n        Exception: Any error propagated from the underlying dataset's loader.\n    \"\"\"\n    entry = self._get_entry(key)\n    return entry.load_arrow(where=where, columns=columns)\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.Catalog.save_arrow","title":"save_arrow","text":"<pre><code>save_arrow(key: str, table: pa.Table) -&gt; None\n</code></pre> <p>Save an Apache Arrow Table to a dataset in the catalog.</p> <p>Resolves the catalog entry identified by <code>key</code> and delegates to :meth:<code>EntryBase.save_arrow</code>. This writes to the entry's configured <code>location</code> with any format-specific save options applied.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The catalog entry name to write to.</p> required <code>table</code> <code>pa.Table</code> <p>The Arrow table to persist.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If <code>key</code> is not present in the catalog.</p> <code>Exception</code> <p>Any error propagated from the underlying dataset's saver.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def save_arrow(self, key: str, table: \"pa.Table\") -&gt; None:\n    \"\"\"Save an Apache Arrow Table to a dataset in the catalog.\n\n    Resolves the catalog entry identified by `key` and delegates to\n    :meth:`EntryBase.save_arrow`. This writes to the entry's configured\n    `location` with any format-specific save options applied.\n\n    Args:\n        key: The catalog entry name to write to.\n        table (pa.Table): The Arrow table to persist.\n\n    Raises:\n        KeyError: If `key` is not present in the catalog.\n        Exception: Any error propagated from the underlying dataset's saver.\n    \"\"\"\n    entry = self._get_entry(key)\n    entry.save_arrow(table)\n</code></pre>"},{"location":"api/catalog/#entries","title":"Entries","text":""},{"location":"api/catalog/#smallcat.catalog.CSVEntry","title":"smallcat.catalog.CSVEntry","text":"<p>               Bases: <code>EntryBase</code></p> <p>Catalog entry describing a CSV dataset.</p> <p>Attributes:</p> Name Type Description <code>file_format</code> <code>Literal['csv']</code> <p>Literal string identifying the file format: <code>'csv'</code>.</p> <code>load_options</code> <code>CSVLoadOptions | None</code> <p>Options controlling CSV reading (see <code>CSVLoadOptions</code>).</p> <code>save_options</code> <code>CSVSaveOptions | None</code> <p>Options controlling CSV writing (see <code>CSVSaveOptions</code>).</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>class CSVEntry(EntryBase):\n    \"\"\"Catalog entry describing a CSV dataset.\n\n    Attributes:\n        file_format: Literal string identifying the file format: `'csv'`.\n        load_options: Options controlling CSV *reading* (see `CSVLoadOptions`).\n        save_options: Options controlling CSV *writing* (see `CSVSaveOptions`).\n    \"\"\"\n\n    file_format: Literal[\"csv\"] = \"csv\"\n    load_options: CSVLoadOptions | None\n    save_options: CSVSaveOptions | None\n\n    def build_dataset(self) -&gt; CSVDataset:\n        \"\"\"Build a :class:`CSVDataset` using this entry's configuration.\n\n        Returns:\n            CSVDataset: A dataset configured with the resolved connection and options.\n        \"\"\"\n        return CSVDataset(\n            conn=self.get_connection(),\n            load_options=self.load_options,\n            save_options=self.save_options,\n        )\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.CSVEntry.file_format","title":"file_format  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>file_format: Literal['csv'] = 'csv'\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.CSVEntry.load_options","title":"load_options  <code>instance-attribute</code>","text":"<pre><code>load_options: CSVLoadOptions | None\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.CSVEntry.save_options","title":"save_options  <code>instance-attribute</code>","text":"<pre><code>save_options: CSVSaveOptions | None\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.CSVEntry.build_dataset","title":"build_dataset","text":"<pre><code>build_dataset() -&gt; CSVDataset\n</code></pre> <p>Build a :class:<code>CSVDataset</code> using this entry's configuration.</p> <p>Returns:</p> Name Type Description <code>CSVDataset</code> <code>CSVDataset</code> <p>A dataset configured with the resolved connection and options.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def build_dataset(self) -&gt; CSVDataset:\n    \"\"\"Build a :class:`CSVDataset` using this entry's configuration.\n\n    Returns:\n        CSVDataset: A dataset configured with the resolved connection and options.\n    \"\"\"\n    return CSVDataset(\n        conn=self.get_connection(),\n        load_options=self.load_options,\n        save_options=self.save_options,\n    )\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.CSVEntry.load_pandas","title":"load_pandas","text":"<pre><code>load_pandas(where: str | None = None, columns: list[str] | None = None) -&gt; pd.DataFrame\n</code></pre> <p>Load this entry's dataset into a pandas DataFrame.</p> <p>This method builds the concrete dataset via :meth:<code>build_dataset</code> and delegates to its <code>load_pandas</code> method using this entry's <code>location</code>. Any dataset-specific load options configured on the entry are respected.</p> <p>Parameters:</p> Name Type Description Default <code>where</code> <code>str | None</code> <p>Optional SQL filter predicate forwarded to the dataset.</p> <code>None</code> <code>columns</code> <code>list[str] | None</code> <p>Optional list of columns to project.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The loaded tabular data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the target path/table at <code>location</code> does not exist.</p> <code>ValueError</code> <p>If the data cannot be parsed as tabular data.</p> <code>Exception</code> <p>Any other error raised by the underlying dataset implementation.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def load_pandas(\n    self,\n    where: str | None = None,\n    columns: list[str] | None = None,\n) -&gt; \"pd.DataFrame\":\n    \"\"\"Load this entry's dataset into a pandas DataFrame.\n\n    This method builds the concrete dataset via :meth:`build_dataset` and\n    delegates to its ``load_pandas`` method using this entry's ``location``.\n    Any dataset-specific load options configured on the entry are respected.\n\n    Args:\n        where: Optional SQL filter predicate forwarded to the dataset.\n        columns: Optional list of columns to project.\n\n    Returns:\n        pd.DataFrame: The loaded tabular data.\n\n    Raises:\n        FileNotFoundError: If the target path/table at ``location`` does not exist.\n        ValueError: If the data cannot be parsed as tabular data.\n        Exception: Any other error raised by the underlying dataset implementation.\n    \"\"\"\n    return self.build_dataset().load_pandas(\n        self.location,\n        where=where,\n        columns=columns,\n    )\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.CSVEntry.save_pandas","title":"save_pandas","text":"<pre><code>save_pandas(df: pd.DataFrame) -&gt; None\n</code></pre> <p>Save a pandas DataFrame to this entry's dataset location.</p> <p>This method builds the concrete dataset via :meth:<code>build_dataset</code> and delegates to its <code>save_pandas</code> method using this entry's <code>location</code>. Any dataset-specific save options configured on the entry are respected.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>The DataFrame to persist.</p> required <p>Raises:</p> Type Description <code>PermissionError</code> <p>If the target cannot be written to.</p> <code>ValueError</code> <p>If the DataFrame is incompatible with the target format/options.</p> <code>Exception</code> <p>Any other error raised by the underlying dataset implementation.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def save_pandas(self, df: \"pd.DataFrame\") -&gt; None:\n    \"\"\"Save a pandas DataFrame to this entry's dataset location.\n\n    This method builds the concrete dataset via :meth:`build_dataset` and\n    delegates to its ``save_pandas`` method using this entry's ``location``.\n    Any dataset-specific save options configured on the entry are respected.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to persist.\n\n    Raises:\n        PermissionError: If the target cannot be written to.\n        ValueError: If the DataFrame is incompatible with the target format/options.\n        Exception: Any other error raised by the underlying dataset implementation.\n    \"\"\"\n    self.build_dataset().save_pandas(self.location, df)\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.ExcelEntry","title":"smallcat.catalog.ExcelEntry","text":"<p>               Bases: <code>EntryBase</code></p> <p>Catalog entry describing an Excel dataset.</p> <p>Attributes:</p> Name Type Description <code>file_format</code> <code>Literal['excel']</code> <p>Literal string identifying the file format: <code>'excel'</code>.</p> <code>load_options</code> <code>ExcelLoadOptions | None</code> <p>Options controlling Excel reading (see <code>ExcelLoadOptions</code>).</p> <code>save_options</code> <code>ExcelSaveOptions | None</code> <p>Options controlling Excel writing (see <code>ExcelSaveOptions</code>).</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>class ExcelEntry(EntryBase):\n    \"\"\"Catalog entry describing an Excel dataset.\n\n    Attributes:\n        file_format: Literal string identifying the file format: `'excel'`.\n        load_options: Options controlling Excel *reading* (see `ExcelLoadOptions`).\n        save_options: Options controlling Excel *writing* (see `ExcelSaveOptions`).\n    \"\"\"\n\n    file_format: Literal[\"excel\"] = \"excel\"\n    load_options: ExcelLoadOptions | None\n    save_options: ExcelSaveOptions | None\n\n    def build_dataset(self) -&gt; ExcelDataset:\n        \"\"\"Build an :class:`ExcelDataset` using this entry's configuration.\n\n        Returns:\n            ExcelDataset: A dataset configured with the resolved connection and options.\n        \"\"\"\n        return ExcelDataset(\n            conn=self.get_connection(),\n            load_options=self.load_options,\n            save_options=self.save_options,\n        )\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.ExcelEntry.file_format","title":"file_format  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>file_format: Literal['excel'] = 'excel'\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.ExcelEntry.load_options","title":"load_options  <code>instance-attribute</code>","text":"<pre><code>load_options: ExcelLoadOptions | None\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.ExcelEntry.save_options","title":"save_options  <code>instance-attribute</code>","text":"<pre><code>save_options: ExcelSaveOptions | None\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.ExcelEntry.build_dataset","title":"build_dataset","text":"<pre><code>build_dataset() -&gt; ExcelDataset\n</code></pre> <p>Build an :class:<code>ExcelDataset</code> using this entry's configuration.</p> <p>Returns:</p> Name Type Description <code>ExcelDataset</code> <code>ExcelDataset</code> <p>A dataset configured with the resolved connection and options.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def build_dataset(self) -&gt; ExcelDataset:\n    \"\"\"Build an :class:`ExcelDataset` using this entry's configuration.\n\n    Returns:\n        ExcelDataset: A dataset configured with the resolved connection and options.\n    \"\"\"\n    return ExcelDataset(\n        conn=self.get_connection(),\n        load_options=self.load_options,\n        save_options=self.save_options,\n    )\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.ExcelEntry.load_pandas","title":"load_pandas","text":"<pre><code>load_pandas(where: str | None = None, columns: list[str] | None = None) -&gt; pd.DataFrame\n</code></pre> <p>Load this entry's dataset into a pandas DataFrame.</p> <p>This method builds the concrete dataset via :meth:<code>build_dataset</code> and delegates to its <code>load_pandas</code> method using this entry's <code>location</code>. Any dataset-specific load options configured on the entry are respected.</p> <p>Parameters:</p> Name Type Description Default <code>where</code> <code>str | None</code> <p>Optional SQL filter predicate forwarded to the dataset.</p> <code>None</code> <code>columns</code> <code>list[str] | None</code> <p>Optional list of columns to project.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The loaded tabular data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the target path/table at <code>location</code> does not exist.</p> <code>ValueError</code> <p>If the data cannot be parsed as tabular data.</p> <code>Exception</code> <p>Any other error raised by the underlying dataset implementation.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def load_pandas(\n    self,\n    where: str | None = None,\n    columns: list[str] | None = None,\n) -&gt; \"pd.DataFrame\":\n    \"\"\"Load this entry's dataset into a pandas DataFrame.\n\n    This method builds the concrete dataset via :meth:`build_dataset` and\n    delegates to its ``load_pandas`` method using this entry's ``location``.\n    Any dataset-specific load options configured on the entry are respected.\n\n    Args:\n        where: Optional SQL filter predicate forwarded to the dataset.\n        columns: Optional list of columns to project.\n\n    Returns:\n        pd.DataFrame: The loaded tabular data.\n\n    Raises:\n        FileNotFoundError: If the target path/table at ``location`` does not exist.\n        ValueError: If the data cannot be parsed as tabular data.\n        Exception: Any other error raised by the underlying dataset implementation.\n    \"\"\"\n    return self.build_dataset().load_pandas(\n        self.location,\n        where=where,\n        columns=columns,\n    )\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.ExcelEntry.save_pandas","title":"save_pandas","text":"<pre><code>save_pandas(df: pd.DataFrame) -&gt; None\n</code></pre> <p>Save a pandas DataFrame to this entry's dataset location.</p> <p>This method builds the concrete dataset via :meth:<code>build_dataset</code> and delegates to its <code>save_pandas</code> method using this entry's <code>location</code>. Any dataset-specific save options configured on the entry are respected.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>The DataFrame to persist.</p> required <p>Raises:</p> Type Description <code>PermissionError</code> <p>If the target cannot be written to.</p> <code>ValueError</code> <p>If the DataFrame is incompatible with the target format/options.</p> <code>Exception</code> <p>Any other error raised by the underlying dataset implementation.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def save_pandas(self, df: \"pd.DataFrame\") -&gt; None:\n    \"\"\"Save a pandas DataFrame to this entry's dataset location.\n\n    This method builds the concrete dataset via :meth:`build_dataset` and\n    delegates to its ``save_pandas`` method using this entry's ``location``.\n    Any dataset-specific save options configured on the entry are respected.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to persist.\n\n    Raises:\n        PermissionError: If the target cannot be written to.\n        ValueError: If the DataFrame is incompatible with the target format/options.\n        Exception: Any other error raised by the underlying dataset implementation.\n    \"\"\"\n    self.build_dataset().save_pandas(self.location, df)\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.ParquetEntry","title":"smallcat.catalog.ParquetEntry","text":"<p>               Bases: <code>EntryBase</code></p> <p>Catalog entry describing a Parquet dataset.</p> <p>Attributes:</p> Name Type Description <code>file_format</code> <code>Literal['parquet']</code> <p>Literal string identifying the file format: <code>'parquet'</code>.</p> <code>load_options</code> <code>ParquetLoadOptions | None</code> <p>Optional configuration controlling Parquet reading behavior (see :class:<code>ParquetLoadOptions</code>).</p> <code>save_options</code> <code>ParquetSaveOptions | None</code> <p>Optional configuration controlling Parquet writing behavior (see :class:<code>ParquetSaveOptions</code>).</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>class ParquetEntry(EntryBase):\n    \"\"\"Catalog entry describing a Parquet dataset.\n\n    Attributes:\n        file_format: Literal string identifying the file format: `'parquet'`.\n        load_options: Optional configuration controlling Parquet *reading*\n            behavior (see :class:`ParquetLoadOptions`).\n        save_options: Optional configuration controlling Parquet *writing*\n            behavior (see :class:`ParquetSaveOptions`).\n    \"\"\"\n\n    file_format: Literal[\"parquet\"] = \"parquet\"\n    load_options: ParquetLoadOptions | None\n    save_options: ParquetSaveOptions | None\n\n    def build_dataset(self) -&gt; ParquetDataset:\n        \"\"\"Build a :class:`ParquetDataset` using this entry's configuration.\n\n        Returns:\n            ParquetDataset: A dataset configured with the resolved connection\n            and Parquet-specific options.\n        \"\"\"\n        return ParquetDataset(\n            conn=self.get_connection(),\n            load_options=self.load_options,\n            save_options=self.save_options,\n        )\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.ParquetEntry.file_format","title":"file_format  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>file_format: Literal['parquet'] = 'parquet'\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.ParquetEntry.load_options","title":"load_options  <code>instance-attribute</code>","text":"<pre><code>load_options: ParquetLoadOptions | None\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.ParquetEntry.save_options","title":"save_options  <code>instance-attribute</code>","text":"<pre><code>save_options: ParquetSaveOptions | None\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.ParquetEntry.build_dataset","title":"build_dataset","text":"<pre><code>build_dataset() -&gt; ParquetDataset\n</code></pre> <p>Build a :class:<code>ParquetDataset</code> using this entry's configuration.</p> <p>Returns:</p> Name Type Description <code>ParquetDataset</code> <code>ParquetDataset</code> <p>A dataset configured with the resolved connection</p> <code>ParquetDataset</code> <p>and Parquet-specific options.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def build_dataset(self) -&gt; ParquetDataset:\n    \"\"\"Build a :class:`ParquetDataset` using this entry's configuration.\n\n    Returns:\n        ParquetDataset: A dataset configured with the resolved connection\n        and Parquet-specific options.\n    \"\"\"\n    return ParquetDataset(\n        conn=self.get_connection(),\n        load_options=self.load_options,\n        save_options=self.save_options,\n    )\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.ParquetEntry.load_pandas","title":"load_pandas","text":"<pre><code>load_pandas(where: str | None = None, columns: list[str] | None = None) -&gt; pd.DataFrame\n</code></pre> <p>Load this entry's dataset into a pandas DataFrame.</p> <p>This method builds the concrete dataset via :meth:<code>build_dataset</code> and delegates to its <code>load_pandas</code> method using this entry's <code>location</code>. Any dataset-specific load options configured on the entry are respected.</p> <p>Parameters:</p> Name Type Description Default <code>where</code> <code>str | None</code> <p>Optional SQL filter predicate forwarded to the dataset.</p> <code>None</code> <code>columns</code> <code>list[str] | None</code> <p>Optional list of columns to project.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The loaded tabular data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the target path/table at <code>location</code> does not exist.</p> <code>ValueError</code> <p>If the data cannot be parsed as tabular data.</p> <code>Exception</code> <p>Any other error raised by the underlying dataset implementation.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def load_pandas(\n    self,\n    where: str | None = None,\n    columns: list[str] | None = None,\n) -&gt; \"pd.DataFrame\":\n    \"\"\"Load this entry's dataset into a pandas DataFrame.\n\n    This method builds the concrete dataset via :meth:`build_dataset` and\n    delegates to its ``load_pandas`` method using this entry's ``location``.\n    Any dataset-specific load options configured on the entry are respected.\n\n    Args:\n        where: Optional SQL filter predicate forwarded to the dataset.\n        columns: Optional list of columns to project.\n\n    Returns:\n        pd.DataFrame: The loaded tabular data.\n\n    Raises:\n        FileNotFoundError: If the target path/table at ``location`` does not exist.\n        ValueError: If the data cannot be parsed as tabular data.\n        Exception: Any other error raised by the underlying dataset implementation.\n    \"\"\"\n    return self.build_dataset().load_pandas(\n        self.location,\n        where=where,\n        columns=columns,\n    )\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.ParquetEntry.save_pandas","title":"save_pandas","text":"<pre><code>save_pandas(df: pd.DataFrame) -&gt; None\n</code></pre> <p>Save a pandas DataFrame to this entry's dataset location.</p> <p>This method builds the concrete dataset via :meth:<code>build_dataset</code> and delegates to its <code>save_pandas</code> method using this entry's <code>location</code>. Any dataset-specific save options configured on the entry are respected.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>The DataFrame to persist.</p> required <p>Raises:</p> Type Description <code>PermissionError</code> <p>If the target cannot be written to.</p> <code>ValueError</code> <p>If the DataFrame is incompatible with the target format/options.</p> <code>Exception</code> <p>Any other error raised by the underlying dataset implementation.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def save_pandas(self, df: \"pd.DataFrame\") -&gt; None:\n    \"\"\"Save a pandas DataFrame to this entry's dataset location.\n\n    This method builds the concrete dataset via :meth:`build_dataset` and\n    delegates to its ``save_pandas`` method using this entry's ``location``.\n    Any dataset-specific save options configured on the entry are respected.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to persist.\n\n    Raises:\n        PermissionError: If the target cannot be written to.\n        ValueError: If the DataFrame is incompatible with the target format/options.\n        Exception: Any other error raised by the underlying dataset implementation.\n    \"\"\"\n    self.build_dataset().save_pandas(self.location, df)\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.DeltaTableEntry","title":"smallcat.catalog.DeltaTableEntry","text":"<p>               Bases: <code>EntryBase</code></p> <p>Catalog entry describing a Delta Lake table dataset.</p> <p>This entry specifies configuration for reading from or writing to Delta Lake tables, typically stored on local or cloud-backed storage. It includes both connection details and Delta-specific load/save options.</p> <p>Attributes:</p> Name Type Description <code>file_format</code> <code>Literal['delta_table']</code> <p>Literal string identifying the file format: <code>'delta_table'</code>.</p> <code>load_options</code> <code>DeltaTableLoadOptions | None</code> <p>Optional configuration controlling Delta table reading behavior (see :class:<code>DeltaTableLoadOptions</code>).</p> <code>save_options</code> <code>DeltaTableSaveOptions | None</code> <p>Optional configuration controlling Delta table writing behavior (see :class:<code>DeltaTableSaveOptions</code>).</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>class DeltaTableEntry(EntryBase):\n    \"\"\"Catalog entry describing a Delta Lake table dataset.\n\n    This entry specifies configuration for reading from or writing to Delta\n    Lake tables, typically stored on local or cloud-backed storage. It includes\n    both connection details and Delta-specific load/save options.\n\n    Attributes:\n        file_format: Literal string identifying the file format: `'delta_table'`.\n        load_options: Optional configuration controlling Delta table *reading*\n            behavior (see :class:`DeltaTableLoadOptions`).\n        save_options: Optional configuration controlling Delta table *writing*\n            behavior (see :class:`DeltaTableSaveOptions`).\n    \"\"\"\n\n    file_format: Literal[\"delta_table\"] = \"delta_table\"\n    load_options: DeltaTableLoadOptions | None\n    save_options: DeltaTableSaveOptions | None\n\n    def build_dataset(self) -&gt; DeltaTableDataset:\n        \"\"\"Build a :class:`DeltaTableDataset` using this entry's configuration.\n\n        Returns:\n            DeltaTableDataset: A dataset configured with the resolved connection\n            and Delta Lake options.\n        \"\"\"\n        return DeltaTableDataset(\n            conn=self.get_connection(),\n            load_options=self.load_options,\n            save_options=self.save_options,\n        )\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.DeltaTableEntry.file_format","title":"file_format  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>file_format: Literal['delta_table'] = 'delta_table'\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.DeltaTableEntry.load_options","title":"load_options  <code>instance-attribute</code>","text":"<pre><code>load_options: DeltaTableLoadOptions | None\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.DeltaTableEntry.save_options","title":"save_options  <code>instance-attribute</code>","text":"<pre><code>save_options: DeltaTableSaveOptions | None\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.DeltaTableEntry.build_dataset","title":"build_dataset","text":"<pre><code>build_dataset() -&gt; DeltaTableDataset\n</code></pre> <p>Build a :class:<code>DeltaTableDataset</code> using this entry's configuration.</p> <p>Returns:</p> Name Type Description <code>DeltaTableDataset</code> <code>DeltaTableDataset</code> <p>A dataset configured with the resolved connection</p> <code>DeltaTableDataset</code> <p>and Delta Lake options.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def build_dataset(self) -&gt; DeltaTableDataset:\n    \"\"\"Build a :class:`DeltaTableDataset` using this entry's configuration.\n\n    Returns:\n        DeltaTableDataset: A dataset configured with the resolved connection\n        and Delta Lake options.\n    \"\"\"\n    return DeltaTableDataset(\n        conn=self.get_connection(),\n        load_options=self.load_options,\n        save_options=self.save_options,\n    )\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.DeltaTableEntry.load_pandas","title":"load_pandas","text":"<pre><code>load_pandas(where: str | None = None, columns: list[str] | None = None) -&gt; pd.DataFrame\n</code></pre> <p>Load this entry's dataset into a pandas DataFrame.</p> <p>This method builds the concrete dataset via :meth:<code>build_dataset</code> and delegates to its <code>load_pandas</code> method using this entry's <code>location</code>. Any dataset-specific load options configured on the entry are respected.</p> <p>Parameters:</p> Name Type Description Default <code>where</code> <code>str | None</code> <p>Optional SQL filter predicate forwarded to the dataset.</p> <code>None</code> <code>columns</code> <code>list[str] | None</code> <p>Optional list of columns to project.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The loaded tabular data.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the target path/table at <code>location</code> does not exist.</p> <code>ValueError</code> <p>If the data cannot be parsed as tabular data.</p> <code>Exception</code> <p>Any other error raised by the underlying dataset implementation.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def load_pandas(\n    self,\n    where: str | None = None,\n    columns: list[str] | None = None,\n) -&gt; \"pd.DataFrame\":\n    \"\"\"Load this entry's dataset into a pandas DataFrame.\n\n    This method builds the concrete dataset via :meth:`build_dataset` and\n    delegates to its ``load_pandas`` method using this entry's ``location``.\n    Any dataset-specific load options configured on the entry are respected.\n\n    Args:\n        where: Optional SQL filter predicate forwarded to the dataset.\n        columns: Optional list of columns to project.\n\n    Returns:\n        pd.DataFrame: The loaded tabular data.\n\n    Raises:\n        FileNotFoundError: If the target path/table at ``location`` does not exist.\n        ValueError: If the data cannot be parsed as tabular data.\n        Exception: Any other error raised by the underlying dataset implementation.\n    \"\"\"\n    return self.build_dataset().load_pandas(\n        self.location,\n        where=where,\n        columns=columns,\n    )\n</code></pre>"},{"location":"api/catalog/#smallcat.catalog.DeltaTableEntry.save_pandas","title":"save_pandas","text":"<pre><code>save_pandas(df: pd.DataFrame) -&gt; None\n</code></pre> <p>Save a pandas DataFrame to this entry's dataset location.</p> <p>This method builds the concrete dataset via :meth:<code>build_dataset</code> and delegates to its <code>save_pandas</code> method using this entry's <code>location</code>. Any dataset-specific save options configured on the entry are respected.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>The DataFrame to persist.</p> required <p>Raises:</p> Type Description <code>PermissionError</code> <p>If the target cannot be written to.</p> <code>ValueError</code> <p>If the DataFrame is incompatible with the target format/options.</p> <code>Exception</code> <p>Any other error raised by the underlying dataset implementation.</p> Source code in <code>src/smallcat/catalog.py</code> <pre><code>def save_pandas(self, df: \"pd.DataFrame\") -&gt; None:\n    \"\"\"Save a pandas DataFrame to this entry's dataset location.\n\n    This method builds the concrete dataset via :meth:`build_dataset` and\n    delegates to its ``save_pandas`` method using this entry's ``location``.\n    Any dataset-specific save options configured on the entry are respected.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to persist.\n\n    Raises:\n        PermissionError: If the target cannot be written to.\n        ValueError: If the DataFrame is incompatible with the target format/options.\n        Exception: Any other error raised by the underlying dataset implementation.\n    \"\"\"\n    self.build_dataset().save_pandas(self.location, df)\n</code></pre>"},{"location":"api/datasets/","title":"Datasets","text":"<p>Dataset constructors for common table/file formats.</p> <p>Re-exports the most common dataset classes so callers can import from <code>smallcat.datasets</code> directly.</p> Available datasets <ul> <li>:class:<code>ParquetDataset</code></li> <li>:class:<code>CSVDataset</code></li> <li>:class:<code>ExcelDataset</code></li> <li>:class:<code>DeltaTableDataset</code></li> <li>:class:<code>BaseDataset</code> (abstract)</li> </ul>"},{"location":"api/datasets/#csv","title":"CSV","text":"<p>CSV dataset using DuckDB's CSV reader/writer.</p> <p>This module defines :class:<code>CSVDataset</code>, a concrete dataset for CSV/TSV/DSV files using DuckDB (<code>read_csv_auto</code> / <code>COPY ... WITH (FORMAT CSV)</code>). All paths are treated as relative to the dataset's base URI.</p> Features <ul> <li>Auto-schema inference (delimiter, header, types) with overrides.</li> <li>Large-file handling via DuckDB streaming.</li> <li>Optional Hive-style partitioning on write.</li> </ul> Example <p>ds = CSVDataset.from_conn_id(\"local_fs\") tbl = ds.load_arrow_table(\"bronze/raw/users.csv\") ds.save_arrow_table(\"silver/users/\", tbl)</p> <p>Typical options (suggested):   * Load: <code>header</code>, <code>delimiter</code>, <code>columns</code>, <code>nullstr</code>, <code>types</code>.   * Save: <code>header</code>, <code>delimiter</code>, <code>partition_by</code>, <code>overwrite</code>.</p> Note <p>An implementation typically builds SQL like: <code>SELECT * FROM read_csv_auto(? , ...options...)</code> for reading and <code>COPY (SELECT * FROM tmp_input) TO ? WITH (FORMAT CSV, ...)</code> for writing.</p>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVDataset","title":"smallcat.datasets.csv_dataset.CSVDataset","text":"<p>               Bases: <code>BaseDataset[CSVLoadOptions, CSVSaveOptions]</code></p> <p>Dataset that reads/writes CSV using DuckDB.</p> <ul> <li>Paths are resolved relative to the dataset's connection base   (local filesystem, <code>gs://</code>, etc.).</li> <li>Reading uses <code>DuckDBPyConnection.read_csv</code> under the hood and returns a   <code>pyarrow.Table</code>.</li> <li>Writing uses <code>Relation.write_csv</code> to materialize a table to CSV.</li> </ul>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVDataset--notes","title":"Notes:","text":"<ul> <li>Use :class:<code>CSVLoadOptions</code> to override auto-detection (separator, header,   per-column types).</li> <li>Use :class:<code>CSVSaveOptions</code> to control delimiter, header, and overwrite behavior.</li> </ul> Source code in <code>src/smallcat/datasets/csv_dataset.py</code> <pre><code>class CSVDataset(BaseDataset[CSVLoadOptions, CSVSaveOptions]):\n    \"\"\"Dataset that reads/writes CSV using DuckDB.\n\n    - **Paths** are resolved relative to the dataset's connection base\n      (local filesystem, `gs://`, etc.).\n    - **Reading** uses `DuckDBPyConnection.read_csv` under the hood and returns a\n      `pyarrow.Table`.\n    - **Writing** uses `Relation.write_csv` to materialize a table to CSV.\n\n    Notes:\n    -----\n    - Use :class:`CSVLoadOptions` to override auto-detection (separator, header,\n      per-column types).\n    - Use :class:`CSVSaveOptions` to control delimiter, header, and overwrite behavior.\n    \"\"\"\n\n    def load_arrow_record_batch_reader(\n        self,\n        path: str,\n        where: str | None = None,\n        columns: list[str] | None = None,\n    ) -&gt; pa.RecordBatchReader:\n        \"\"\"Stream CSV rows as `RecordBatch`es with an optional filter.\"\"\"\n        full_uri = self._full_uri(path)\n        query = self._build_query(\"data\", columns, where)\n        with self._duckdb_conn() as con:\n            rel = con.read_csv(full_uri, **self.load_options_dict())\n            return rel.query(\"data\", query).fetch_record_batch()\n\n    def save_arrow_table(self, path: str, table: pa.Table) -&gt; None:\n        \"\"\"Write a PyArrow Table to CSV.\n\n        Parameters\n        ----------\n        path\n            Destination path (file or pattern) relative to the connection base.\n            Compression is inferred from the extension (e.g. '.gz', '.zst').\n        table\n            The Arrow table to write.\n\n        Raises:\n        ------\n        duckdb.IOException\n            If the destination is not writable.\n        \"\"\"\n        full_uri = self._full_uri(path)\n        with self._duckdb_conn() as con:\n            con.register(\"tmp_input\", table)\n            con.sql(\"SELECT * FROM tmp_input\").write_csv(\n                full_uri,\n                **self.save_options_dict(),\n            )\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVDataset.load_arrow_record_batch_reader","title":"load_arrow_record_batch_reader","text":"<pre><code>load_arrow_record_batch_reader(path: str, where: str | None = None, columns: list[str] | None = None) -&gt; pa.RecordBatchReader\n</code></pre> <p>Stream CSV rows as <code>RecordBatch</code>es with an optional filter.</p> Source code in <code>src/smallcat/datasets/csv_dataset.py</code> <pre><code>def load_arrow_record_batch_reader(\n    self,\n    path: str,\n    where: str | None = None,\n    columns: list[str] | None = None,\n) -&gt; pa.RecordBatchReader:\n    \"\"\"Stream CSV rows as `RecordBatch`es with an optional filter.\"\"\"\n    full_uri = self._full_uri(path)\n    query = self._build_query(\"data\", columns, where)\n    with self._duckdb_conn() as con:\n        rel = con.read_csv(full_uri, **self.load_options_dict())\n        return rel.query(\"data\", query).fetch_record_batch()\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVDataset.save_arrow_table","title":"save_arrow_table","text":"<pre><code>save_arrow_table(path: str, table: pa.Table) -&gt; None\n</code></pre> <p>Write a PyArrow Table to CSV.</p>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVDataset.save_arrow_table--parameters","title":"Parameters","text":"<p>path     Destination path (file or pattern) relative to the connection base.     Compression is inferred from the extension (e.g. '.gz', '.zst'). table     The Arrow table to write.</p>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVDataset.save_arrow_table--raises","title":"Raises:","text":"<p>duckdb.IOException     If the destination is not writable.</p> Source code in <code>src/smallcat/datasets/csv_dataset.py</code> <pre><code>def save_arrow_table(self, path: str, table: pa.Table) -&gt; None:\n    \"\"\"Write a PyArrow Table to CSV.\n\n    Parameters\n    ----------\n    path\n        Destination path (file or pattern) relative to the connection base.\n        Compression is inferred from the extension (e.g. '.gz', '.zst').\n    table\n        The Arrow table to write.\n\n    Raises:\n    ------\n    duckdb.IOException\n        If the destination is not writable.\n    \"\"\"\n    full_uri = self._full_uri(path)\n    with self._duckdb_conn() as con:\n        con.register(\"tmp_input\", table)\n        con.sql(\"SELECT * FROM tmp_input\").write_csv(\n            full_uri,\n            **self.save_options_dict(),\n        )\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVDataset.from_conn_id","title":"from_conn_id  <code>classmethod</code>","text":"<pre><code>from_conn_id(conn_id: str, *, load_options: L | None = None, save_options: S | None = None) -&gt; BaseDataset[L, S]\n</code></pre> <p>Construct an instance by looking up an Airflow connection ID.</p> <p>Uses <code>airflow.hooks.base.BaseHook</code> (or the SDK alternative) to fetch the connection and then calls the class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>Airflow connection ID to resolve.</p> required <code>load_options</code> <code>L | None</code> <p>Optional load options model.</p> <code>None</code> <code>save_options</code> <code>S | None</code> <p>Optional save options model.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseDataset[L, S]</code> <p>A fully constructed <code>BaseDataset</code> subclass instance.</p> Source code in <code>src/smallcat/datasets/base_dataset.py</code> <pre><code>@classmethod\ndef from_conn_id(\n    cls: type[\"BaseDataset[L, S]\"],\n    conn_id: str,\n    *,\n    load_options: L | None = None,\n    save_options: S | None = None,\n) -&gt; \"BaseDataset[L, S]\":\n    \"\"\"Construct an instance by looking up an Airflow connection ID.\n\n    Uses `airflow.hooks.base.BaseHook` (or the SDK alternative) to fetch\n    the connection and then calls the class constructor.\n\n    Args:\n      conn_id: Airflow connection ID to resolve.\n      load_options: Optional load options model.\n      save_options: Optional save options model.\n\n    Returns:\n      A fully constructed `BaseDataset` subclass instance.\n    \"\"\"\n    if BaseHook is None:\n        raise RuntimeError(\"Airflow not available. Install smallcat[airflow]\")  # noqa: TRY003, EM101\n\n    conn = BaseHook.get_connection(conn_id)\n    return cls(conn=conn, load_options=load_options, save_options=save_options)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVDataset.load_pandas","title":"load_pandas","text":"<pre><code>load_pandas(path: str, where: str | None = None, columns: list[str] | None = None) -&gt; pd.DataFrame\n</code></pre> <p>Load data as a pandas DataFrame.</p> <p>This is a convenience wrapper over <code>load_arrow_table</code> and pushes down filters when provided.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative dataset path.</p> required <code>where</code> <code>str | None</code> <p>Optional SQL filter predicate injected into the query.</p> <code>None</code> <code>columns</code> <code>list[str] | None</code> <p>Optional list of columns to project.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A pandas <code>DataFrame</code>.</p> Source code in <code>src/smallcat/datasets/base_dataset.py</code> <pre><code>def load_pandas(\n    self,\n    path: str,\n    where: str | None = None,\n    columns: list[str] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Load data as a pandas DataFrame.\n\n    This is a convenience wrapper over `load_arrow_table` and pushes down\n    filters when provided.\n\n    Args:\n      path: Relative dataset path.\n      where: Optional SQL filter predicate injected into the query.\n      columns: Optional list of columns to project.\n\n    Returns:\n      A pandas `DataFrame`.\n    \"\"\"\n    arrow_table = self.load_arrow_table(\n        path=path,\n        where=where,\n        columns=columns,\n    )\n    return arrow_table.to_pandas()\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVDataset.save_pandas","title":"save_pandas","text":"<pre><code>save_pandas(path: str, df: pd.DataFrame) -&gt; None\n</code></pre> <p>Persist a pandas DataFrame.</p> <p>Converts the DataFrame to a <code>pyarrow.Table</code> and delegates to <code>save_arrow_table</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative dataset path.</p> required <code>df</code> <code>pd.DataFrame</code> <p>DataFrame to persist.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/smallcat/datasets/base_dataset.py</code> <pre><code>def save_pandas(self, path: str, df: pd.DataFrame) -&gt; None:\n    \"\"\"Persist a pandas DataFrame.\n\n    Converts the DataFrame to a `pyarrow.Table` and delegates to\n    `save_arrow_table`.\n\n    Args:\n        path: Relative dataset path.\n        df: DataFrame to persist.\n\n    Returns:\n        None\n    \"\"\"\n    arrow_table = pa.Table.from_pandas(df)\n    self.save_arrow_table(path, table=arrow_table)\n</code></pre>"},{"location":"api/datasets/#options","title":"Options","text":""},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVLoadOptions","title":"smallcat.datasets.csv_dataset.CSVLoadOptions","text":"<p>               Bases: <code>BaseModel</code></p> <p>Options that control how CSV files are read.</p> <p>These mirror DuckDB's <code>read_csv_auto</code> parameters we expose. All fields are optional; unset values defer to DuckDB defaults.</p>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVLoadOptions--attributes","title":"Attributes:","text":"<p>columns     Optional mapping of column names to logical types     (e.g. {\"id\": \"INTEGER\", \"amount\": \"DOUBLE\"}) used to override     DuckDB's type inference when auto-detect is not good enough. sep     Field separator (e.g. \",\", \"|\", \"\\t\"). If None, DuckDB will try to detect it. header     Whether the first row contains column names. If None, DuckDB will detect. sample_size     Number of rows to sample for schema detection. If None, DuckDB default applies. all_varchar     If True, read all columns as VARCHAR (string). Useful when types are messy.</p> Source code in <code>src/smallcat/datasets/csv_dataset.py</code> <pre><code>class CSVLoadOptions(BaseModel):\n    r\"\"\"Options that control how CSV files are *read*.\n\n    These mirror DuckDB's `read_csv_auto` parameters we expose.\n    All fields are optional; unset values defer to DuckDB defaults.\n\n    Attributes:\n    ----------\n    columns\n        Optional mapping of column names to logical types\n        (e.g. {\"id\": \"INTEGER\", \"amount\": \"DOUBLE\"}) used to override\n        DuckDB's type inference when auto-detect is not good enough.\n    sep\n        Field separator (e.g. \",\", \"|\", \"\\t\"). If None, DuckDB will try to detect it.\n    header\n        Whether the first row contains column names. If None, DuckDB will detect.\n    sample_size\n        Number of rows to sample for schema detection. If None, DuckDB default applies.\n    all_varchar\n        If True, read all columns as VARCHAR (string). Useful when types are messy.\n    \"\"\"\n\n    columns: Mapping[str, str] | None = Field(\n        None,\n        description=\"Override inferred types per column, e.g. {'id': 'INTEGER'}.\",\n    )\n    sep: str | None = Field(\n        None,\n        description=\"Field separator (e.g. ',', '|', '\\\\t'); auto-detected if None.\",\n    )\n    header: bool | None = Field(\n        None,\n        description=\"Whether the first row is a header; auto-detected if None.\",\n    )\n    sample_size: int | None = Field(\n        None,\n        description=\"Rows to sample for inference; DuckDB default if None.\",\n    )\n    all_varchar: bool | None = Field(\n        None,\n        description=\"If True, read all columns as VARCHAR.\",\n    )\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVLoadOptions.columns","title":"columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>columns: Mapping[str, str] | None = Field(None, description=\"Override inferred types per column, e.g. {'id': 'INTEGER'}.\")\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVLoadOptions.sep","title":"sep  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sep: str | None = Field(None, description=\"Field separator (e.g. ',', '|', '\\\\t'); auto-detected if None.\")\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVLoadOptions.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: bool | None = Field(None, description='Whether the first row is a header; auto-detected if None.')\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVLoadOptions.sample_size","title":"sample_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sample_size: int | None = Field(None, description='Rows to sample for inference; DuckDB default if None.')\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVLoadOptions.all_varchar","title":"all_varchar  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>all_varchar: bool | None = Field(None, description='If True, read all columns as VARCHAR.')\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVSaveOptions","title":"smallcat.datasets.csv_dataset.CSVSaveOptions","text":"<p>               Bases: <code>BaseModel</code></p> <p>Options that control how CSV files are written.</p>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVSaveOptions--attributes","title":"Attributes:","text":"<p>header     Whether to write a header row with column names. sep     Field separator to use when writing (e.g. ',', '|', '\\t'). overwrite     If True, allow overwriting existing files at the destination.     Compression is inferred from the file extension ('.gz', '.zst', \u2026).</p> Source code in <code>src/smallcat/datasets/csv_dataset.py</code> <pre><code>class CSVSaveOptions(BaseModel):\n    r\"\"\"Options that control how CSV files are *written*.\n\n    Attributes:\n    ----------\n    header\n        Whether to write a header row with column names.\n    sep\n        Field separator to use when writing (e.g. ',', '|', '\\t').\n    overwrite\n        If True, allow overwriting existing files at the destination.\n        Compression is *inferred from the file extension* ('.gz', '.zst', \u2026).\n    \"\"\"\n\n    header: bool | None = Field(\n        None,\n        description=\"Write a header row with column names.\",\n    )\n    sep: str | None = Field(\n        None,\n        description=\"Field separator to use (e.g. ',', '|', r'\\t').\",\n    )\n    # compression is inferred from extension (.gz, .zst, \u2026) don't expose unless you must\n    overwrite: bool | None = Field(\n        None,\n        description=\"If True, overwrite existing files at the destination.\",\n    )\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVSaveOptions.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: bool | None = Field(None, description='Write a header row with column names.')\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVSaveOptions.sep","title":"sep  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sep: str | None = Field(None, description=\"Field separator to use (e.g. ',', '|', r'\\t').\")\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.csv_dataset.CSVSaveOptions.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool | None = Field(None, description='If True, overwrite existing files at the destination.')\n</code></pre>"},{"location":"api/datasets/#excel","title":"Excel","text":"<p>Excel (.xlsx) dataset via DuckDB's <code>excel</code> extension.</p> <p>This module provides :class:<code>ExcelDataset</code> for reading/writing .xlsx files (legacy .xls is not supported). Paths are relative to the configured base URI; the DuckDB <code>excel</code> extension is installed/loaded at runtime.</p> Capabilities <ul> <li>Read a whole sheet or an A1 range with optional header handling.</li> <li>Coerce empty columns or all columns to VARCHAR for schema stability.</li> <li>Write Arrow tables to a specific sheet (with optional header row).</li> </ul> Example <p>ds = ExcelDataset.from_conn_id(\"fs_conn\") tbl = ds.load_arrow_table(\"inputs/budget.xlsx\")  # first sheet by default ds.save_arrow_table(\"outputs/budget_out.xlsx\", tbl)</p> Options <ul> <li><code>ExcelLoadOptions</code>: header, sheet, range, all_varchar, empty_as_varchar.</li> <li><code>ExcelSaveOptions</code>: header, sheet.</li> </ul>"},{"location":"api/datasets/#smallcat.datasets.excel_dataset.ExcelDataset","title":"smallcat.datasets.excel_dataset.ExcelDataset","text":"<p>               Bases: <code>BaseDataset[ExcelLoadOptions, ExcelSaveOptions]</code></p> <p>Reads and writes .xlsx files via DuckDB's <code>excel</code> extension.</p> Notes <ul> <li>Legacy .xls format is not supported.</li> <li>Paths are treated as relative to this dataset's base URI (e.g., <code>file://</code> or   <code>gs://</code>); use the connection extras to set the base.</li> </ul> Source code in <code>src/smallcat/datasets/excel_dataset.py</code> <pre><code>class ExcelDataset(BaseDataset[ExcelLoadOptions, ExcelSaveOptions]):\n    \"\"\"Reads and writes **.xlsx** files via DuckDB's `excel` extension.\n\n    Notes:\n      * Legacy **.xls** format is **not** supported.\n      * Paths are treated as relative to this dataset's base URI (e.g., `file://` or\n        `gs://`); use the connection extras to set the base.\n    \"\"\"\n\n    def load_arrow_record_batch_reader(\n        self,\n        path: str,\n        where: str | None = None,\n        columns: list[str] | None = None,\n    ) -&gt; pa.RecordBatchReader:\n        \"\"\"Stream .xlsx rows as record batches with an optional filter.\"\"\"\n        full_uri = self._full_uri(path)\n        query = self._build_query(\"data\", columns, where)\n        with self._duckdb_conn() as con:\n            con.install_extension(\"excel\")\n            con.load_extension(\"excel\")\n            lo = self.load_options_dict()\n            args_sql = \"\"\n            if header := lo.get(\"header\"):\n                args_sql += f\", header = {str(header).lower()}\"\n            if sheet := lo.get(\"sheet\"):\n                args_sql += f\", sheet = '{sheet}'\"\n            if _range := lo.get(\"range\"):\n                args_sql += f\", range = '{_range}'\"\n            if all_varchar := lo.get(\"all_varchar\"):\n                args_sql += f\", all_varchar = {str(all_varchar).lower()}\"\n            if empty_as_varchar := lo.get(\"empty_as_varchar\"):\n                args_sql += f\", empty_as_varchar = {str(empty_as_varchar).lower()}\"\n            sql = re.sub(\n                r\"FROM\\s+data\",\n                f\"from read_xlsx(?{args_sql})\",\n                query,\n                flags=re.IGNORECASE,\n            )\n            return con.execute(sql, [full_uri]).fetch_record_batch()\n\n    def save_arrow_table(self, path: str, table: pa.Table) -&gt; None:\n        \"\"\"Write a PyArrow table to an .xlsx file.\n\n        Args:\n          path: Relative path of the output .xlsx file (joined under the dataset base).\n          table: The `pyarrow.Table` to write.\n\n        Notes:\n          Uses DuckDB's `COPY ... TO ... WITH (FORMAT xlsx ...)` from the\n          `excel` extension. Save-time options are translated into COPY options.\n        \"\"\"\n        full_uri = self._full_uri(path)\n        with self._duckdb_conn() as con:\n            con.install_extension(\"excel\")\n            con.load_extension(\"excel\")\n            lo = self.save_options_dict()\n            args_sql = \"\"\n            if header := lo.get(\"header\"):\n                args_sql += f\", HEADER {str(header).lower()}\"\n            if sheet := lo.get(\"sheet\"):\n                args_sql += f\", SHEET '{sheet}'\"\n            con.register(\"tmp_input\", table)\n            query = f\"copy (select * from tmp_input) TO ? WITH (FORMAT xlsx{args_sql})\"  # noqa: S608\n            con.execute(query, [full_uri]).fetch_arrow_table()\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.excel_dataset.ExcelDataset.load_arrow_record_batch_reader","title":"load_arrow_record_batch_reader","text":"<pre><code>load_arrow_record_batch_reader(path: str, where: str | None = None, columns: list[str] | None = None) -&gt; pa.RecordBatchReader\n</code></pre> <p>Stream .xlsx rows as record batches with an optional filter.</p> Source code in <code>src/smallcat/datasets/excel_dataset.py</code> <pre><code>def load_arrow_record_batch_reader(\n    self,\n    path: str,\n    where: str | None = None,\n    columns: list[str] | None = None,\n) -&gt; pa.RecordBatchReader:\n    \"\"\"Stream .xlsx rows as record batches with an optional filter.\"\"\"\n    full_uri = self._full_uri(path)\n    query = self._build_query(\"data\", columns, where)\n    with self._duckdb_conn() as con:\n        con.install_extension(\"excel\")\n        con.load_extension(\"excel\")\n        lo = self.load_options_dict()\n        args_sql = \"\"\n        if header := lo.get(\"header\"):\n            args_sql += f\", header = {str(header).lower()}\"\n        if sheet := lo.get(\"sheet\"):\n            args_sql += f\", sheet = '{sheet}'\"\n        if _range := lo.get(\"range\"):\n            args_sql += f\", range = '{_range}'\"\n        if all_varchar := lo.get(\"all_varchar\"):\n            args_sql += f\", all_varchar = {str(all_varchar).lower()}\"\n        if empty_as_varchar := lo.get(\"empty_as_varchar\"):\n            args_sql += f\", empty_as_varchar = {str(empty_as_varchar).lower()}\"\n        sql = re.sub(\n            r\"FROM\\s+data\",\n            f\"from read_xlsx(?{args_sql})\",\n            query,\n            flags=re.IGNORECASE,\n        )\n        return con.execute(sql, [full_uri]).fetch_record_batch()\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.excel_dataset.ExcelDataset.save_arrow_table","title":"save_arrow_table","text":"<pre><code>save_arrow_table(path: str, table: pa.Table) -&gt; None\n</code></pre> <p>Write a PyArrow table to an .xlsx file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative path of the output .xlsx file (joined under the dataset base).</p> required <code>table</code> <code>pa.Table</code> <p>The <code>pyarrow.Table</code> to write.</p> required Notes <p>Uses DuckDB's <code>COPY ... TO ... WITH (FORMAT xlsx ...)</code> from the <code>excel</code> extension. Save-time options are translated into COPY options.</p> Source code in <code>src/smallcat/datasets/excel_dataset.py</code> <pre><code>def save_arrow_table(self, path: str, table: pa.Table) -&gt; None:\n    \"\"\"Write a PyArrow table to an .xlsx file.\n\n    Args:\n      path: Relative path of the output .xlsx file (joined under the dataset base).\n      table: The `pyarrow.Table` to write.\n\n    Notes:\n      Uses DuckDB's `COPY ... TO ... WITH (FORMAT xlsx ...)` from the\n      `excel` extension. Save-time options are translated into COPY options.\n    \"\"\"\n    full_uri = self._full_uri(path)\n    with self._duckdb_conn() as con:\n        con.install_extension(\"excel\")\n        con.load_extension(\"excel\")\n        lo = self.save_options_dict()\n        args_sql = \"\"\n        if header := lo.get(\"header\"):\n            args_sql += f\", HEADER {str(header).lower()}\"\n        if sheet := lo.get(\"sheet\"):\n            args_sql += f\", SHEET '{sheet}'\"\n        con.register(\"tmp_input\", table)\n        query = f\"copy (select * from tmp_input) TO ? WITH (FORMAT xlsx{args_sql})\"  # noqa: S608\n        con.execute(query, [full_uri]).fetch_arrow_table()\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.excel_dataset.ExcelDataset.from_conn_id","title":"from_conn_id  <code>classmethod</code>","text":"<pre><code>from_conn_id(conn_id: str, *, load_options: L | None = None, save_options: S | None = None) -&gt; BaseDataset[L, S]\n</code></pre> <p>Construct an instance by looking up an Airflow connection ID.</p> <p>Uses <code>airflow.hooks.base.BaseHook</code> (or the SDK alternative) to fetch the connection and then calls the class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>Airflow connection ID to resolve.</p> required <code>load_options</code> <code>L | None</code> <p>Optional load options model.</p> <code>None</code> <code>save_options</code> <code>S | None</code> <p>Optional save options model.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseDataset[L, S]</code> <p>A fully constructed <code>BaseDataset</code> subclass instance.</p> Source code in <code>src/smallcat/datasets/base_dataset.py</code> <pre><code>@classmethod\ndef from_conn_id(\n    cls: type[\"BaseDataset[L, S]\"],\n    conn_id: str,\n    *,\n    load_options: L | None = None,\n    save_options: S | None = None,\n) -&gt; \"BaseDataset[L, S]\":\n    \"\"\"Construct an instance by looking up an Airflow connection ID.\n\n    Uses `airflow.hooks.base.BaseHook` (or the SDK alternative) to fetch\n    the connection and then calls the class constructor.\n\n    Args:\n      conn_id: Airflow connection ID to resolve.\n      load_options: Optional load options model.\n      save_options: Optional save options model.\n\n    Returns:\n      A fully constructed `BaseDataset` subclass instance.\n    \"\"\"\n    if BaseHook is None:\n        raise RuntimeError(\"Airflow not available. Install smallcat[airflow]\")  # noqa: TRY003, EM101\n\n    conn = BaseHook.get_connection(conn_id)\n    return cls(conn=conn, load_options=load_options, save_options=save_options)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.excel_dataset.ExcelDataset.load_pandas","title":"load_pandas","text":"<pre><code>load_pandas(path: str, where: str | None = None, columns: list[str] | None = None) -&gt; pd.DataFrame\n</code></pre> <p>Load data as a pandas DataFrame.</p> <p>This is a convenience wrapper over <code>load_arrow_table</code> and pushes down filters when provided.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative dataset path.</p> required <code>where</code> <code>str | None</code> <p>Optional SQL filter predicate injected into the query.</p> <code>None</code> <code>columns</code> <code>list[str] | None</code> <p>Optional list of columns to project.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A pandas <code>DataFrame</code>.</p> Source code in <code>src/smallcat/datasets/base_dataset.py</code> <pre><code>def load_pandas(\n    self,\n    path: str,\n    where: str | None = None,\n    columns: list[str] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Load data as a pandas DataFrame.\n\n    This is a convenience wrapper over `load_arrow_table` and pushes down\n    filters when provided.\n\n    Args:\n      path: Relative dataset path.\n      where: Optional SQL filter predicate injected into the query.\n      columns: Optional list of columns to project.\n\n    Returns:\n      A pandas `DataFrame`.\n    \"\"\"\n    arrow_table = self.load_arrow_table(\n        path=path,\n        where=where,\n        columns=columns,\n    )\n    return arrow_table.to_pandas()\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.excel_dataset.ExcelDataset.save_pandas","title":"save_pandas","text":"<pre><code>save_pandas(path: str, df: pd.DataFrame) -&gt; None\n</code></pre> <p>Persist a pandas DataFrame.</p> <p>Converts the DataFrame to a <code>pyarrow.Table</code> and delegates to <code>save_arrow_table</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative dataset path.</p> required <code>df</code> <code>pd.DataFrame</code> <p>DataFrame to persist.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/smallcat/datasets/base_dataset.py</code> <pre><code>def save_pandas(self, path: str, df: pd.DataFrame) -&gt; None:\n    \"\"\"Persist a pandas DataFrame.\n\n    Converts the DataFrame to a `pyarrow.Table` and delegates to\n    `save_arrow_table`.\n\n    Args:\n        path: Relative dataset path.\n        df: DataFrame to persist.\n\n    Returns:\n        None\n    \"\"\"\n    arrow_table = pa.Table.from_pandas(df)\n    self.save_arrow_table(path, table=arrow_table)\n</code></pre>"},{"location":"api/datasets/#options_1","title":"Options","text":""},{"location":"api/datasets/#smallcat.datasets.excel_dataset.ExcelLoadOptions","title":"smallcat.datasets.excel_dataset.ExcelLoadOptions","text":"<p>               Bases: <code>BaseModel</code></p> <p>Options that control how an .xlsx file is read.</p> <p>Attributes:</p> Name Type Description <code>header</code> <code>bool | None</code> <p>If True, treat the first row as column headers.</p> <code>sheet</code> <code>str | None</code> <p>Optional worksheet name to read. If omitted, the first sheet is used.</p> <code>range</code> <code>str | None</code> <p>Excel A1-style range to read (e.g., \"A1:D100\").      If omitted, the full sheet is read.</p> <code>all_varchar</code> <code>bool | None</code> <p>If True, coerce all columns to VARCHAR (strings).</p> <code>empty_as_varchar</code> <code>bool | None</code> <p>If True, treat empty columns as VARCHAR instead of NULL/typed.</p> Source code in <code>src/smallcat/datasets/excel_dataset.py</code> <pre><code>class ExcelLoadOptions(BaseModel):\n    \"\"\"Options that control how an .xlsx file is read.\n\n    Attributes:\n      header: If True, treat the first row as column headers.\n      sheet: Optional worksheet name to read. If omitted, the first sheet is used.\n      range: Excel A1-style range to read (e.g., \"A1:D100\").\n             If omitted, the full sheet is read.\n      all_varchar: If True, coerce all columns to VARCHAR (strings).\n      empty_as_varchar: If True, treat empty columns as VARCHAR instead of NULL/typed.\n    \"\"\"\n\n    header: bool | None = Field(None)\n    sheet: str | None = Field(None)\n    range: str | None = Field(None)\n    all_varchar: bool | None = Field(None)\n    empty_as_varchar: bool | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.excel_dataset.ExcelLoadOptions.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: bool | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.excel_dataset.ExcelLoadOptions.sheet","title":"sheet  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sheet: str | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.excel_dataset.ExcelLoadOptions.range","title":"range  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>range: str | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.excel_dataset.ExcelLoadOptions.all_varchar","title":"all_varchar  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>all_varchar: bool | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.excel_dataset.ExcelLoadOptions.empty_as_varchar","title":"empty_as_varchar  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>empty_as_varchar: bool | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.excel_dataset.ExcelSaveOptions","title":"smallcat.datasets.excel_dataset.ExcelSaveOptions","text":"<p>               Bases: <code>BaseModel</code></p> <p>Options that control how an Arrow table is written to .xlsx.</p> <p>Attributes:</p> Name Type Description <code>header</code> <code>bool | None</code> <p>If True, include column headers in the output file.</p> <code>sheet</code> <code>str | None</code> <p>Optional worksheet name to write into (created if missing).</p> Source code in <code>src/smallcat/datasets/excel_dataset.py</code> <pre><code>class ExcelSaveOptions(BaseModel):\n    \"\"\"Options that control how an Arrow table is written to .xlsx.\n\n    Attributes:\n      header: If True, include column headers in the output file.\n      sheet: Optional worksheet name to write into (created if missing).\n    \"\"\"\n\n    header: bool | None = Field(None)\n    sheet: str | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.excel_dataset.ExcelSaveOptions.header","title":"header  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>header: bool | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.excel_dataset.ExcelSaveOptions.sheet","title":"sheet  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sheet: str | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#parquet","title":"Parquet","text":"<p>Parquet dataset backed by DuckDB.</p> <p>This module provides :class:<code>ParquetDataset</code>, a concrete implementation of :class:<code>~smallcat.datasets.base_dataset.BaseDataset</code> that reads/writes Parquet via DuckDB. Paths passed to public methods are relative to the configured base (e.g., <code>file://</code> or <code>gs://</code>).</p> Features <ul> <li>Read from a single file, directory, or glob pattern.</li> <li>Hive partition discovery and schema union (optional).</li> <li>Write with optional partitioning and overwrite.</li> </ul> Example <p>ds = ParquetDataset.from_conn_id(\"gcs_conn\") tbl = ds.load_arrow_table(\"bronze/events/*/.parquet\") ds.save_arrow_table(\"silver/events/\", tbl)</p> Related options <ul> <li><code>ParquetLoadOptions</code>: binary_as_string, hive_partitioning, union_by_name...</li> <li><code>ParquetSaveOptions</code>: overwrite, partition_by, write_partition_columns.</li> </ul>"},{"location":"api/datasets/#smallcat.datasets.parquet_dataset.ParquetDataset","title":"smallcat.datasets.parquet_dataset.ParquetDataset","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Parquet dataset backed by DuckDB's Parquet reader/writer.</p> <p>Paths passed to public methods are treated as relative to the dataset's configured base (e.g., <code>file://</code> or <code>gs://</code>). Reads return a PyArrow table.</p> Notes <ul> <li>You can pass a single file, a directory (e.g., <code>/path/**.parquet</code>),   or any glob DuckDB understands.</li> </ul> Source code in <code>src/smallcat/datasets/parquet_dataset.py</code> <pre><code>class ParquetDataset(BaseDataset):\n    \"\"\"Parquet dataset backed by DuckDB's Parquet reader/writer.\n\n    Paths passed to public methods are treated as **relative** to the dataset's\n    configured base (e.g., `file://` or `gs://`). Reads return a PyArrow\n    table.\n\n    Notes:\n      * You can pass a single file, a directory (e.g., `/path/**.parquet`),\n        or any glob DuckDB understands.\n    \"\"\"\n\n    def load_arrow_record_batch_reader(\n        self,\n        path: str,\n        where: str | None = None,\n        columns: list[str] | None = None,\n    ) -&gt; pa.RecordBatchReader:\n        \"\"\"Stream Parquet rows as record batches with an optional filter.\"\"\"\n        full_uri = self._full_uri(path)\n        query = self._build_query(\"data\", columns, where)\n        with self._duckdb_conn() as con:\n            rel = con.read_parquet(full_uri, **self.load_options_dict())\n            return rel.query(\"data\", query).fetch_record_batch()\n\n    def save_arrow_table(self, path: str, table: pa.Table) -&gt; None:\n        \"\"\"Write a PyArrow table to Parquet.\n\n        Args:\n          path: Relative output path (file or directory) joined under the\n            dataset base URI.\n          table: The `pyarrow.Table` to write.\n\n        Notes:\n          Uses `Relation.write_parquet` with parameters from\n          `save_options_dict()`.\n        \"\"\"\n        full_uri = self._full_uri(path)\n        with self._duckdb_conn() as con:\n            con.register(\"tmp_input\", table)\n            con.sql(\"SELECT * FROM tmp_input\").write_parquet(\n                full_uri,\n                **self.save_options_dict(),\n            )\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.parquet_dataset.ParquetDataset.load_arrow_record_batch_reader","title":"load_arrow_record_batch_reader","text":"<pre><code>load_arrow_record_batch_reader(path: str, where: str | None = None, columns: list[str] | None = None) -&gt; pa.RecordBatchReader\n</code></pre> <p>Stream Parquet rows as record batches with an optional filter.</p> Source code in <code>src/smallcat/datasets/parquet_dataset.py</code> <pre><code>def load_arrow_record_batch_reader(\n    self,\n    path: str,\n    where: str | None = None,\n    columns: list[str] | None = None,\n) -&gt; pa.RecordBatchReader:\n    \"\"\"Stream Parquet rows as record batches with an optional filter.\"\"\"\n    full_uri = self._full_uri(path)\n    query = self._build_query(\"data\", columns, where)\n    with self._duckdb_conn() as con:\n        rel = con.read_parquet(full_uri, **self.load_options_dict())\n        return rel.query(\"data\", query).fetch_record_batch()\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.parquet_dataset.ParquetDataset.save_arrow_table","title":"save_arrow_table","text":"<pre><code>save_arrow_table(path: str, table: pa.Table) -&gt; None\n</code></pre> <p>Write a PyArrow table to Parquet.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative output path (file or directory) joined under the dataset base URI.</p> required <code>table</code> <code>pa.Table</code> <p>The <code>pyarrow.Table</code> to write.</p> required Notes <p>Uses <code>Relation.write_parquet</code> with parameters from <code>save_options_dict()</code>.</p> Source code in <code>src/smallcat/datasets/parquet_dataset.py</code> <pre><code>def save_arrow_table(self, path: str, table: pa.Table) -&gt; None:\n    \"\"\"Write a PyArrow table to Parquet.\n\n    Args:\n      path: Relative output path (file or directory) joined under the\n        dataset base URI.\n      table: The `pyarrow.Table` to write.\n\n    Notes:\n      Uses `Relation.write_parquet` with parameters from\n      `save_options_dict()`.\n    \"\"\"\n    full_uri = self._full_uri(path)\n    with self._duckdb_conn() as con:\n        con.register(\"tmp_input\", table)\n        con.sql(\"SELECT * FROM tmp_input\").write_parquet(\n            full_uri,\n            **self.save_options_dict(),\n        )\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.parquet_dataset.ParquetDataset.from_conn_id","title":"from_conn_id  <code>classmethod</code>","text":"<pre><code>from_conn_id(conn_id: str, *, load_options: L | None = None, save_options: S | None = None) -&gt; BaseDataset[L, S]\n</code></pre> <p>Construct an instance by looking up an Airflow connection ID.</p> <p>Uses <code>airflow.hooks.base.BaseHook</code> (or the SDK alternative) to fetch the connection and then calls the class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>Airflow connection ID to resolve.</p> required <code>load_options</code> <code>L | None</code> <p>Optional load options model.</p> <code>None</code> <code>save_options</code> <code>S | None</code> <p>Optional save options model.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseDataset[L, S]</code> <p>A fully constructed <code>BaseDataset</code> subclass instance.</p> Source code in <code>src/smallcat/datasets/base_dataset.py</code> <pre><code>@classmethod\ndef from_conn_id(\n    cls: type[\"BaseDataset[L, S]\"],\n    conn_id: str,\n    *,\n    load_options: L | None = None,\n    save_options: S | None = None,\n) -&gt; \"BaseDataset[L, S]\":\n    \"\"\"Construct an instance by looking up an Airflow connection ID.\n\n    Uses `airflow.hooks.base.BaseHook` (or the SDK alternative) to fetch\n    the connection and then calls the class constructor.\n\n    Args:\n      conn_id: Airflow connection ID to resolve.\n      load_options: Optional load options model.\n      save_options: Optional save options model.\n\n    Returns:\n      A fully constructed `BaseDataset` subclass instance.\n    \"\"\"\n    if BaseHook is None:\n        raise RuntimeError(\"Airflow not available. Install smallcat[airflow]\")  # noqa: TRY003, EM101\n\n    conn = BaseHook.get_connection(conn_id)\n    return cls(conn=conn, load_options=load_options, save_options=save_options)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.parquet_dataset.ParquetDataset.load_pandas","title":"load_pandas","text":"<pre><code>load_pandas(path: str, where: str | None = None, columns: list[str] | None = None) -&gt; pd.DataFrame\n</code></pre> <p>Load data as a pandas DataFrame.</p> <p>This is a convenience wrapper over <code>load_arrow_table</code> and pushes down filters when provided.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative dataset path.</p> required <code>where</code> <code>str | None</code> <p>Optional SQL filter predicate injected into the query.</p> <code>None</code> <code>columns</code> <code>list[str] | None</code> <p>Optional list of columns to project.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A pandas <code>DataFrame</code>.</p> Source code in <code>src/smallcat/datasets/base_dataset.py</code> <pre><code>def load_pandas(\n    self,\n    path: str,\n    where: str | None = None,\n    columns: list[str] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Load data as a pandas DataFrame.\n\n    This is a convenience wrapper over `load_arrow_table` and pushes down\n    filters when provided.\n\n    Args:\n      path: Relative dataset path.\n      where: Optional SQL filter predicate injected into the query.\n      columns: Optional list of columns to project.\n\n    Returns:\n      A pandas `DataFrame`.\n    \"\"\"\n    arrow_table = self.load_arrow_table(\n        path=path,\n        where=where,\n        columns=columns,\n    )\n    return arrow_table.to_pandas()\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.parquet_dataset.ParquetDataset.save_pandas","title":"save_pandas","text":"<pre><code>save_pandas(path: str, df: pd.DataFrame) -&gt; None\n</code></pre> <p>Persist a pandas DataFrame.</p> <p>Converts the DataFrame to a <code>pyarrow.Table</code> and delegates to <code>save_arrow_table</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative dataset path.</p> required <code>df</code> <code>pd.DataFrame</code> <p>DataFrame to persist.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/smallcat/datasets/base_dataset.py</code> <pre><code>def save_pandas(self, path: str, df: pd.DataFrame) -&gt; None:\n    \"\"\"Persist a pandas DataFrame.\n\n    Converts the DataFrame to a `pyarrow.Table` and delegates to\n    `save_arrow_table`.\n\n    Args:\n        path: Relative dataset path.\n        df: DataFrame to persist.\n\n    Returns:\n        None\n    \"\"\"\n    arrow_table = pa.Table.from_pandas(df)\n    self.save_arrow_table(path, table=arrow_table)\n</code></pre>"},{"location":"api/datasets/#options_2","title":"Options","text":""},{"location":"api/datasets/#smallcat.datasets.parquet_dataset.ParquetLoadOptions","title":"smallcat.datasets.parquet_dataset.ParquetLoadOptions","text":"<p>               Bases: <code>BaseModel</code></p> <p>Options that control how Parquet is read via DuckDB.</p> <p>Attributes:</p> Name Type Description <code>binary_as_string</code> <code>bool | None</code> <p>If True, interpret BINARY columns as strings.</p> <code>file_row_number</code> <code>bool | None</code> <p>If True, include a synthetic row-number column per file.</p> <code>hive_partitioning</code> <code>bool | None</code> <p>If True, parse Hive-style directory partitions.</p> <code>union_by_name</code> <code>bool | None</code> <p>If True, align/union schemas by column name across files.</p> Source code in <code>src/smallcat/datasets/parquet_dataset.py</code> <pre><code>class ParquetLoadOptions(BaseModel):\n    \"\"\"Options that control how Parquet is read via DuckDB.\n\n    Attributes:\n      binary_as_string: If True, interpret BINARY columns as strings.\n      file_row_number: If True, include a synthetic row-number column per file.\n      hive_partitioning: If True, parse Hive-style directory partitions.\n      union_by_name: If True, align/union schemas by column name across files.\n    \"\"\"\n\n    binary_as_string: bool | None = Field(None)\n    file_row_number: bool | None = Field(None)\n    hive_partitioning: bool | None = Field(None)\n    union_by_name: bool | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.parquet_dataset.ParquetLoadOptions.binary_as_string","title":"binary_as_string  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>binary_as_string: bool | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.parquet_dataset.ParquetLoadOptions.file_row_number","title":"file_row_number  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>file_row_number: bool | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.parquet_dataset.ParquetLoadOptions.hive_partitioning","title":"hive_partitioning  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>hive_partitioning: bool | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.parquet_dataset.ParquetLoadOptions.union_by_name","title":"union_by_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>union_by_name: bool | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.parquet_dataset.ParquetSaveOptions","title":"smallcat.datasets.parquet_dataset.ParquetSaveOptions","text":"<p>               Bases: <code>BaseModel</code></p> <p>Options that control how Parquet is written via DuckDB.</p> <p>Attributes:</p> Name Type Description <code>overwrite</code> <code>bool | None</code> <p>If True, overwrite existing output.</p> <code>partition_by</code> <code>list[str] | None</code> <p>Columns to partition by (Hive-style layout).</p> <code>write_partition_columns</code> <code>bool | None</code> <p>If True, also materialize partition cols in files.</p> Source code in <code>src/smallcat/datasets/parquet_dataset.py</code> <pre><code>class ParquetSaveOptions(BaseModel):\n    \"\"\"Options that control how Parquet is written via DuckDB.\n\n    Attributes:\n      overwrite: If True, overwrite existing output.\n      partition_by: Columns to partition by (Hive-style layout).\n      write_partition_columns: If True, also materialize partition cols in files.\n    \"\"\"\n\n    overwrite: bool | None = Field(None)\n    partition_by: list[str] | None = Field(None)\n    write_partition_columns: bool | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.parquet_dataset.ParquetSaveOptions.overwrite","title":"overwrite  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>overwrite: bool | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.parquet_dataset.ParquetSaveOptions.partition_by","title":"partition_by  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>partition_by: list[str] | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.parquet_dataset.ParquetSaveOptions.write_partition_columns","title":"write_partition_columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>write_partition_columns: bool | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#delta-table","title":"Delta Table","text":"<p>Delta Lake dataset using delta-rs (deltalake) with Smallcat.</p> <p>This module implements :class:<code>DeltaTableDataset</code>, a Delta Lake reader/writer powered by <code>deltalake</code> (delta-rs). It resolves relative paths against the connection base (e.g., <code>gs://bucket/prefix</code>) and returns/accepts Arrow tables.</p> Storage backends <ul> <li>Local filesystem (<code>fs</code>) - no extra options.</li> <li>Google Cloud Storage (<code>google_cloud_platform</code>) - credentials derived from   connection extras: <code>keyfile_dict</code> / <code>keyfile</code> / <code>key_path</code>.</li> <li>Databricks - minimal env vars exported (workspace URL and token).</li> </ul> Example <p>ds = DeltaTableDataset.from_conn_id(\"gcs_delta\") tbl = ds.load_arrow_table(\"bronze/events_delta\") ds.save_arrow_table(\"silver/events_delta\", tbl)</p> Options <ul> <li><code>DeltaTableLoadOptions</code>: version, without_files, log_buffer_size.</li> <li><code>DeltaTableSaveOptions</code>: mode, partition_by, schema_mode.</li> </ul> Notes <p>For Databricks, this module sets: <code>DATABRICKS_WORKSPACE_URL</code> and <code>DATABRICKS_ACCESS_TOKEN</code> before access.</p>"},{"location":"api/datasets/#smallcat.datasets.delta_table_dataset.DeltaTableDataset","title":"smallcat.datasets.delta_table_dataset.DeltaTableDataset","text":"<p>               Bases: <code>BaseDataset[DeltaTableLoadOptions, DeltaTableSaveOptions]</code></p> <p>Delta Lake dataset that reads/writes via delta-rs (DeltaTable / write_deltalake).</p> <p>Paths passed to public methods are treated as relative to the dataset's configured base (e.g., local <code>file://</code> or <code>gs://</code>). Reads return a PyArrow table.</p> Notes <ul> <li>For Google Cloud Storage, credentials are derived from the connection's   extras (e.g., <code>keyfile_dict</code>, <code>keyfile</code>, or <code>key_path</code>).</li> <li>For <code>conn_type == \"databricks\"</code>, environment variables are set to   support Databricks-hosted Delta.</li> </ul> Source code in <code>src/smallcat/datasets/delta_table_dataset.py</code> <pre><code>class DeltaTableDataset(BaseDataset[DeltaTableLoadOptions, DeltaTableSaveOptions]):\n    \"\"\"Delta Lake dataset that reads/writes via delta-rs (DeltaTable / write_deltalake).\n\n    Paths passed to public methods are treated as **relative** to the dataset's\n    configured base (e.g., local `file://` or `gs://`). Reads return a\n    PyArrow table.\n\n    Notes:\n      * For Google Cloud Storage, credentials are derived from the connection's\n        extras (e.g., `keyfile_dict`, `keyfile`, or `key_path`).\n      * For `conn_type == \"databricks\"`, environment variables are set to\n        support Databricks-hosted Delta.\n    \"\"\"\n\n    def _delta_storage_options(self) -&gt; dict:\n        \"\"\"Build `storage_options` for delta-rs reads/writes.\n\n        The options are derived from the active connection:\n          * `fs` (local): returns `{}`.\n          * `google_cloud_platform`: uses one of:\n              - `extras.keyfile_dict` (dict or JSON string)\n              - `extras.keyfile` (raw JSON string)\n              - `extras.key_path` (path on worker)\n\n        Returns:\n          A mapping suitable for the `storage_options` parameter used by\n          `deltalake.DeltaTable` and `deltalake.write_deltalake`. For GCS,\n          keys include one of:\n            * `google_service_account_key` (serialized JSON)\n            * `google_service_account` (path to keyfile)\n\n        Raises:\n          ValueError: If the connection type is not supported.\n        \"\"\"\n        if self.conn.conn_type not in [\"fs\", \"google_cloud_platform\"]:\n            msg = f\"Storage options not implemented for type {self.conn.conn_type}\"\n            raise ValueError(\n                msg,\n            )\n        x = self.extras\n\n        # keyfile_dict can be dict or JSON string\n        kfd = x.get(\"keyfile_dict\")\n        if isinstance(kfd, str):\n            try:\n                kfd = json.loads(kfd)\n            except json.JSONDecodeError:\n                # If it's not JSON, ignore and fall through\n                kfd = None\n\n        if isinstance(kfd, dict) and kfd:\n            # Provide serialized key via 'google_service_account_key'\n            return {\"google_service_account_key\": json.dumps(kfd)}\n\n        if x.get(\"keyfile\"):  # raw JSON string\n            return {\"google_service_account_key\": x[\"keyfile\"]}\n\n        if x.get(\"key_path\"):  # path on worker\n            return {\"google_service_account\": x[\"key_path\"]}\n\n        return {}\n\n    def _set_databricks_acces_variables(self) -&gt; None:\n        \"\"\"Export minimal environment variables for Databricks-hosted Delta.\n\n        Sets:\n          * `DATABRICKS_WORKSPACE_URL` from `self.conn.host`\n          * `DATABRICKS_ACCESS_TOKEN` from `self.conn.password`\n\n        Notes:\n          These variables are used by delta-rs when accessing Databricks.\n        \"\"\"\n        if self.conn.host is None or self.conn.password is None:\n            msg = \"Databricks connection requires both host and password.\"\n            raise ValueError(msg)\n        os.environ[\"DATABRICKS_WORKSPACE_URL\"] = self.conn.host\n        os.environ[\"DATABRICKS_ACCESS_TOKEN\"] = self.conn.password\n\n    def load_arrow_record_batch_reader(\n        self,\n        path: str,\n        where: str | None = None,\n        columns: list[str] | None = None,\n    ) -&gt; pa.RecordBatchReader:\n        \"\"\"Stream Delta Lake rows via DuckDB with an optional filter.\"\"\"\n        full_uri = self._full_uri(path)\n        if self.conn.conn_type == \"databricks\":\n            self._set_databricks_acces_variables()\n            raise NotImplementedError\n\n        query = self._build_query(\"data\", columns, where)\n        storage_options = self._delta_storage_options()\n        dt = DeltaTable(\n            full_uri,\n            storage_options=storage_options,\n            **self.load_options_dict(),\n        )\n        dataset = dt.to_pyarrow_dataset()\n        with self._duckdb_conn() as con:\n            con.register(\"data\", dataset)\n            return con.sql(query).fetch_record_batch()\n\n    def save_arrow_table(self, path: str, table: pa.Table) -&gt; None:\n        \"\"\"Write a PyArrow table to Delta Lake using delta-rs.\n\n        Args:\n          path: Relative path to the target Delta table (joined under the\n            dataset's base URI).\n          table: The `pyarrow.Table` to write.\n\n        Notes:\n          * If `conn_type == \"databricks\"`, this method sets Databricks\n            environment variables via `_set_databricks_acces_variables`.\n            (The write is otherwise handled by delta-rs for non-Databricks.)\n        \"\"\"\n        \"\"\"True Delta write using delta-rs.\"\"\"\n        table_uri = self._full_uri(path)\n        if self.conn.conn_type == \"databricks\":\n            self._set_databricks_acces_variables()\n        else:\n            storage_options = self._delta_storage_options()\n            try:\n                engine = (\n                    \"rust\"\n                    if self.save_options is not None\n                    and self.save_options.schema_mode == SchemaMode.MERGE\n                    else \"pyarrow\"\n                )\n                write_deltalake(\n                    table_or_uri=table_uri,\n                    data=table,\n                    storage_options=storage_options,\n                    engine=engine,\n                    **self.save_options_dict(),\n                )\n            except TypeError:\n                # Newer versions use the rust engine and don't take the engine parameter\n                write_deltalake(\n                    table_or_uri=table_uri,\n                    data=table,\n                    storage_options=storage_options,\n                    **self.save_options_dict(),\n                )\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.delta_table_dataset.DeltaTableDataset._delta_storage_options","title":"_delta_storage_options","text":"<pre><code>_delta_storage_options() -&gt; dict\n</code></pre> <p>Build <code>storage_options</code> for delta-rs reads/writes.</p> The options are derived from the active connection <ul> <li><code>fs</code> (local): returns <code>{}</code>.</li> <li><code>google_cloud_platform</code>: uses one of:<ul> <li><code>extras.keyfile_dict</code> (dict or JSON string)</li> <li><code>extras.keyfile</code> (raw JSON string)</li> <li><code>extras.key_path</code> (path on worker)</li> </ul> </li> </ul> <p>Returns:</p> Type Description <code>dict</code> <p>A mapping suitable for the <code>storage_options</code> parameter used by</p> <code>dict</code> <p><code>deltalake.DeltaTable</code> and <code>deltalake.write_deltalake</code>. For GCS,</p> <code>dict</code> <p>keys include one of: * <code>google_service_account_key</code> (serialized JSON) * <code>google_service_account</code> (path to keyfile)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the connection type is not supported.</p> Source code in <code>src/smallcat/datasets/delta_table_dataset.py</code> <pre><code>def _delta_storage_options(self) -&gt; dict:\n    \"\"\"Build `storage_options` for delta-rs reads/writes.\n\n    The options are derived from the active connection:\n      * `fs` (local): returns `{}`.\n      * `google_cloud_platform`: uses one of:\n          - `extras.keyfile_dict` (dict or JSON string)\n          - `extras.keyfile` (raw JSON string)\n          - `extras.key_path` (path on worker)\n\n    Returns:\n      A mapping suitable for the `storage_options` parameter used by\n      `deltalake.DeltaTable` and `deltalake.write_deltalake`. For GCS,\n      keys include one of:\n        * `google_service_account_key` (serialized JSON)\n        * `google_service_account` (path to keyfile)\n\n    Raises:\n      ValueError: If the connection type is not supported.\n    \"\"\"\n    if self.conn.conn_type not in [\"fs\", \"google_cloud_platform\"]:\n        msg = f\"Storage options not implemented for type {self.conn.conn_type}\"\n        raise ValueError(\n            msg,\n        )\n    x = self.extras\n\n    # keyfile_dict can be dict or JSON string\n    kfd = x.get(\"keyfile_dict\")\n    if isinstance(kfd, str):\n        try:\n            kfd = json.loads(kfd)\n        except json.JSONDecodeError:\n            # If it's not JSON, ignore and fall through\n            kfd = None\n\n    if isinstance(kfd, dict) and kfd:\n        # Provide serialized key via 'google_service_account_key'\n        return {\"google_service_account_key\": json.dumps(kfd)}\n\n    if x.get(\"keyfile\"):  # raw JSON string\n        return {\"google_service_account_key\": x[\"keyfile\"]}\n\n    if x.get(\"key_path\"):  # path on worker\n        return {\"google_service_account\": x[\"key_path\"]}\n\n    return {}\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.delta_table_dataset.DeltaTableDataset._set_databricks_acces_variables","title":"_set_databricks_acces_variables","text":"<pre><code>_set_databricks_acces_variables() -&gt; None\n</code></pre> <p>Export minimal environment variables for Databricks-hosted Delta.</p> Sets <ul> <li><code>DATABRICKS_WORKSPACE_URL</code> from <code>self.conn.host</code></li> <li><code>DATABRICKS_ACCESS_TOKEN</code> from <code>self.conn.password</code></li> </ul> Notes <p>These variables are used by delta-rs when accessing Databricks.</p> Source code in <code>src/smallcat/datasets/delta_table_dataset.py</code> <pre><code>def _set_databricks_acces_variables(self) -&gt; None:\n    \"\"\"Export minimal environment variables for Databricks-hosted Delta.\n\n    Sets:\n      * `DATABRICKS_WORKSPACE_URL` from `self.conn.host`\n      * `DATABRICKS_ACCESS_TOKEN` from `self.conn.password`\n\n    Notes:\n      These variables are used by delta-rs when accessing Databricks.\n    \"\"\"\n    if self.conn.host is None or self.conn.password is None:\n        msg = \"Databricks connection requires both host and password.\"\n        raise ValueError(msg)\n    os.environ[\"DATABRICKS_WORKSPACE_URL\"] = self.conn.host\n    os.environ[\"DATABRICKS_ACCESS_TOKEN\"] = self.conn.password\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.delta_table_dataset.DeltaTableDataset.load_arrow_record_batch_reader","title":"load_arrow_record_batch_reader","text":"<pre><code>load_arrow_record_batch_reader(path: str, where: str | None = None, columns: list[str] | None = None) -&gt; pa.RecordBatchReader\n</code></pre> <p>Stream Delta Lake rows via DuckDB with an optional filter.</p> Source code in <code>src/smallcat/datasets/delta_table_dataset.py</code> <pre><code>def load_arrow_record_batch_reader(\n    self,\n    path: str,\n    where: str | None = None,\n    columns: list[str] | None = None,\n) -&gt; pa.RecordBatchReader:\n    \"\"\"Stream Delta Lake rows via DuckDB with an optional filter.\"\"\"\n    full_uri = self._full_uri(path)\n    if self.conn.conn_type == \"databricks\":\n        self._set_databricks_acces_variables()\n        raise NotImplementedError\n\n    query = self._build_query(\"data\", columns, where)\n    storage_options = self._delta_storage_options()\n    dt = DeltaTable(\n        full_uri,\n        storage_options=storage_options,\n        **self.load_options_dict(),\n    )\n    dataset = dt.to_pyarrow_dataset()\n    with self._duckdb_conn() as con:\n        con.register(\"data\", dataset)\n        return con.sql(query).fetch_record_batch()\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.delta_table_dataset.DeltaTableDataset.save_arrow_table","title":"save_arrow_table","text":"<pre><code>save_arrow_table(path: str, table: pa.Table) -&gt; None\n</code></pre> <p>Write a PyArrow table to Delta Lake using delta-rs.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative path to the target Delta table (joined under the dataset's base URI).</p> required <code>table</code> <code>pa.Table</code> <p>The <code>pyarrow.Table</code> to write.</p> required Notes <ul> <li>If <code>conn_type == \"databricks\"</code>, this method sets Databricks   environment variables via <code>_set_databricks_acces_variables</code>.   (The write is otherwise handled by delta-rs for non-Databricks.)</li> </ul> Source code in <code>src/smallcat/datasets/delta_table_dataset.py</code> <pre><code>def save_arrow_table(self, path: str, table: pa.Table) -&gt; None:\n    \"\"\"Write a PyArrow table to Delta Lake using delta-rs.\n\n    Args:\n      path: Relative path to the target Delta table (joined under the\n        dataset's base URI).\n      table: The `pyarrow.Table` to write.\n\n    Notes:\n      * If `conn_type == \"databricks\"`, this method sets Databricks\n        environment variables via `_set_databricks_acces_variables`.\n        (The write is otherwise handled by delta-rs for non-Databricks.)\n    \"\"\"\n    \"\"\"True Delta write using delta-rs.\"\"\"\n    table_uri = self._full_uri(path)\n    if self.conn.conn_type == \"databricks\":\n        self._set_databricks_acces_variables()\n    else:\n        storage_options = self._delta_storage_options()\n        try:\n            engine = (\n                \"rust\"\n                if self.save_options is not None\n                and self.save_options.schema_mode == SchemaMode.MERGE\n                else \"pyarrow\"\n            )\n            write_deltalake(\n                table_or_uri=table_uri,\n                data=table,\n                storage_options=storage_options,\n                engine=engine,\n                **self.save_options_dict(),\n            )\n        except TypeError:\n            # Newer versions use the rust engine and don't take the engine parameter\n            write_deltalake(\n                table_or_uri=table_uri,\n                data=table,\n                storage_options=storage_options,\n                **self.save_options_dict(),\n            )\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.delta_table_dataset.DeltaTableDataset.from_conn_id","title":"from_conn_id  <code>classmethod</code>","text":"<pre><code>from_conn_id(conn_id: str, *, load_options: L | None = None, save_options: S | None = None) -&gt; BaseDataset[L, S]\n</code></pre> <p>Construct an instance by looking up an Airflow connection ID.</p> <p>Uses <code>airflow.hooks.base.BaseHook</code> (or the SDK alternative) to fetch the connection and then calls the class constructor.</p> <p>Parameters:</p> Name Type Description Default <code>conn_id</code> <code>str</code> <p>Airflow connection ID to resolve.</p> required <code>load_options</code> <code>L | None</code> <p>Optional load options model.</p> <code>None</code> <code>save_options</code> <code>S | None</code> <p>Optional save options model.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseDataset[L, S]</code> <p>A fully constructed <code>BaseDataset</code> subclass instance.</p> Source code in <code>src/smallcat/datasets/base_dataset.py</code> <pre><code>@classmethod\ndef from_conn_id(\n    cls: type[\"BaseDataset[L, S]\"],\n    conn_id: str,\n    *,\n    load_options: L | None = None,\n    save_options: S | None = None,\n) -&gt; \"BaseDataset[L, S]\":\n    \"\"\"Construct an instance by looking up an Airflow connection ID.\n\n    Uses `airflow.hooks.base.BaseHook` (or the SDK alternative) to fetch\n    the connection and then calls the class constructor.\n\n    Args:\n      conn_id: Airflow connection ID to resolve.\n      load_options: Optional load options model.\n      save_options: Optional save options model.\n\n    Returns:\n      A fully constructed `BaseDataset` subclass instance.\n    \"\"\"\n    if BaseHook is None:\n        raise RuntimeError(\"Airflow not available. Install smallcat[airflow]\")  # noqa: TRY003, EM101\n\n    conn = BaseHook.get_connection(conn_id)\n    return cls(conn=conn, load_options=load_options, save_options=save_options)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.delta_table_dataset.DeltaTableDataset.load_pandas","title":"load_pandas","text":"<pre><code>load_pandas(path: str, where: str | None = None, columns: list[str] | None = None) -&gt; pd.DataFrame\n</code></pre> <p>Load data as a pandas DataFrame.</p> <p>This is a convenience wrapper over <code>load_arrow_table</code> and pushes down filters when provided.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative dataset path.</p> required <code>where</code> <code>str | None</code> <p>Optional SQL filter predicate injected into the query.</p> <code>None</code> <code>columns</code> <code>list[str] | None</code> <p>Optional list of columns to project.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>A pandas <code>DataFrame</code>.</p> Source code in <code>src/smallcat/datasets/base_dataset.py</code> <pre><code>def load_pandas(\n    self,\n    path: str,\n    where: str | None = None,\n    columns: list[str] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Load data as a pandas DataFrame.\n\n    This is a convenience wrapper over `load_arrow_table` and pushes down\n    filters when provided.\n\n    Args:\n      path: Relative dataset path.\n      where: Optional SQL filter predicate injected into the query.\n      columns: Optional list of columns to project.\n\n    Returns:\n      A pandas `DataFrame`.\n    \"\"\"\n    arrow_table = self.load_arrow_table(\n        path=path,\n        where=where,\n        columns=columns,\n    )\n    return arrow_table.to_pandas()\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.delta_table_dataset.DeltaTableDataset.save_pandas","title":"save_pandas","text":"<pre><code>save_pandas(path: str, df: pd.DataFrame) -&gt; None\n</code></pre> <p>Persist a pandas DataFrame.</p> <p>Converts the DataFrame to a <code>pyarrow.Table</code> and delegates to <code>save_arrow_table</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Relative dataset path.</p> required <code>df</code> <code>pd.DataFrame</code> <p>DataFrame to persist.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/smallcat/datasets/base_dataset.py</code> <pre><code>def save_pandas(self, path: str, df: pd.DataFrame) -&gt; None:\n    \"\"\"Persist a pandas DataFrame.\n\n    Converts the DataFrame to a `pyarrow.Table` and delegates to\n    `save_arrow_table`.\n\n    Args:\n        path: Relative dataset path.\n        df: DataFrame to persist.\n\n    Returns:\n        None\n    \"\"\"\n    arrow_table = pa.Table.from_pandas(df)\n    self.save_arrow_table(path, table=arrow_table)\n</code></pre>"},{"location":"api/datasets/#options_3","title":"Options","text":""},{"location":"api/datasets/#smallcat.datasets.delta_table_dataset.DeltaTableLoadOptions","title":"smallcat.datasets.delta_table_dataset.DeltaTableLoadOptions","text":"<p>               Bases: <code>BaseModel</code></p> <p>Options controlling how a Delta table is read.</p> <p>Attributes:</p> Name Type Description <code>version</code> <code>int | None</code> <p>Optional table version to read.</p> <code>without_files</code> <code>bool | None</code> <p>If True, skip listing data files (metadata-only read).</p> <code>log_buffer_size</code> <code>int | None</code> <p>Buffer size for reading Delta logs.</p> Source code in <code>src/smallcat/datasets/delta_table_dataset.py</code> <pre><code>class DeltaTableLoadOptions(BaseModel):\n    \"\"\"Options controlling how a Delta table is read.\n\n    Attributes:\n      version: Optional table version to read.\n      without_files: If True, skip listing data files (metadata-only read).\n      log_buffer_size: Buffer size for reading Delta logs.\n    \"\"\"\n\n    version: int | None = Field(None)\n    without_files: bool | None = Field(None)\n    log_buffer_size: int | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.delta_table_dataset.DeltaTableLoadOptions.version","title":"version  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>version: int | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.delta_table_dataset.DeltaTableLoadOptions.without_files","title":"without_files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>without_files: bool | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.delta_table_dataset.DeltaTableLoadOptions.log_buffer_size","title":"log_buffer_size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>log_buffer_size: int | None = Field(None)\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.delta_table_dataset.DeltaTableSaveOptions","title":"smallcat.datasets.delta_table_dataset.DeltaTableSaveOptions","text":"<p>               Bases: <code>BaseModel</code></p> <p>Options controlling how a Delta table is written.</p> <p>Attributes:</p> Name Type Description <code>mode</code> <code>WriteMode | None</code> <p>Write mode to apply if the table exists.</p> <code>partition_by</code> <code>list[str] | None</code> <p>Columns to partition by (Hive-style directory layout).</p> <code>schema_mode</code> <code>SchemaMode | None</code> <p>Strategy to reconcile schema differences during write.</p> Source code in <code>src/smallcat/datasets/delta_table_dataset.py</code> <pre><code>class DeltaTableSaveOptions(BaseModel):\n    \"\"\"Options controlling how a Delta table is written.\n\n    Attributes:\n      mode: Write mode to apply if the table exists.\n      partition_by: Columns to partition by (Hive-style directory layout).\n      schema_mode: Strategy to reconcile schema differences during write.\n    \"\"\"\n\n    mode: WriteMode | None = Field(None, description=\"Write mode for existing tables.\")\n    partition_by: list[str] | None = Field(\n        None,\n        description=\"Columns to partition by (Hive-style directory layout).\",\n    )\n    schema_mode: SchemaMode | None = Field(\n        None,\n        description=\"How to handle schema differences on write.\",\n    )\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.delta_table_dataset.DeltaTableSaveOptions.mode","title":"mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mode: WriteMode | None = Field(None, description='Write mode for existing tables.')\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.delta_table_dataset.DeltaTableSaveOptions.partition_by","title":"partition_by  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>partition_by: list[str] | None = Field(None, description='Columns to partition by (Hive-style directory layout).')\n</code></pre>"},{"location":"api/datasets/#smallcat.datasets.delta_table_dataset.DeltaTableSaveOptions.schema_mode","title":"schema_mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>schema_mode: SchemaMode | None = Field(None, description='How to handle schema differences on write.')\n</code></pre>"},{"location":"examples/airflow/","title":"Smallcat Airflow","text":"<p>Use Smallcat with Airflow. In this example we'll work locally by reading connections and variables from the Local Filesystem Backend. In production we could deploy the same DAG but the variables and connections would be defined in the database or a Secret Backend</p>"},{"location":"examples/airflow/#setup","title":"Setup","text":""},{"location":"examples/airflow/#install-smallcat","title":"Install smallcat","text":"<p>You can install smallcat with airflow dependencies.</p> <pre><code>pip install smallcat\npip install apache-airflow\n</code></pre> <p>or</p> <pre><code>uv add smallcat\nuv add apache-airflow\n</code></pre>"},{"location":"examples/airflow/#point-airflow-to-local-secrets-files","title":"Point Airflow to local secrets files","text":"<p>Configure the Local Filesystem Secrets Backend:</p> <pre><code>AIRFLOW__SECRETS__BACKENDS=airflow.secrets.local_filesystem.LocalFilesystemBackend\nAIRFLOW__SECRETS__BACKEND_KWARGS='{\"connections_file_path\":\"/path/to/connections.yaml\",\"variables_file_path\":\"/path/to/variables.yaml\"}'\n</code></pre> <p>Replace the paths with real locations.</p>"},{"location":"examples/airflow/#define-a-connection-local-filesystem","title":"Define a connection (local filesystem)","text":"<p><code>connections.yaml</code>:</p> <pre><code>local_filesystem:\n  conn_type: fs\n  extra:\n    base_path: /tmp\n</code></pre>"},{"location":"examples/airflow/#define-a-sample-catalog-variable","title":"Define a sample catalog variable","text":"<p><code>variables.yaml</code>:</p> <pre><code>my_catalog:\n  entries:\n    # Raw daily weather data\n    foo:\n      file_format: csv\n      connection: local_filesystem\n      location: foo.csv\n      save_options:\n        header: true\n      load_options:\n        header: true\n\n    # Transformed output we\u2019ll write\n    foo_processed:\n      file_format: csv\n      connection: local_filesystem\n      location: foo_processed.csv\n      save_options:\n        header: true\n      load_options:\n        header: true\n</code></pre> <p>This declares two datasets under <code>/tmp</code>:</p> <ul> <li><code>foo.csv</code> (input)</li> <li><code>foo_processed.csv</code> (output).</li> </ul>"},{"location":"examples/airflow/#usage","title":"Usage","text":""},{"location":"examples/airflow/#example-dag","title":"Example DAG","text":"<p>The example pulls the Seattle weather CSV, stores it as foo, then creates a filtered/augmented dataset foo_processed:</p> <ul> <li>parse dates</li> <li>keep 2012-01-01 to 2012-03-31</li> <li>keep only rainy/drizzly days</li> <li>add temp_range = temp_max - temp_min</li> </ul> <pre><code>from datetime import datetime\nfrom airflow import DAG\nfrom airflow.decorators import task\nimport pandas as pd\n\nfrom smallcat.catalog import Catalog\n\n\nSOURCE_URL = \"https://raw.githubusercontent.com/vega/vega-datasets/master/data/seattle-weather.csv\"\n\ndef transform(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"This would usually be a function that you import and you have tested in notebooks\"\"\"\n    return (\n        df\n        .assign(date=lambda d: pd.to_datetime(d[\"date\"]))\n        .loc[lambda d: d[\"date\"].between(\"2012-01-01\", \"2012-03-31\")\n                      &amp; d[\"weather\"].isin([\"rain\", \"drizzle\"])]\n        .assign(temp_range=lambda d: d[\"temp_max\"] - d[\"temp_min\"])\n        .loc[:, [\"date\", \"precipitation\", \"temp_max\", \"temp_min\", \"weather\", \"temp_range\"]]\n        .copy()\n    )\n\nwith DAG(\n    dag_id=\"example_smallcat_weather\",\n    start_date=datetime(2024, 1, 1),\n    schedule=\"@once\",\n    catchup=False,\n) as dag:\n    @task\n    def get_input_data():\n        \"\"\"We can use the catalog to save input data.\n        Only used so we have some data to work with.\n        This would usually be a Data Engineering process\"\"\"\n        catalog = Catalog.from_airflow_variable(\"my_catalog\")\n        raw_df = pd.read_csv(SOURCE_URL)\n        catalog.save_pandas(\"foo\", raw_df)\n\n    @task\n    def ds_pipeline():\n        catalog = Catalog.from_airflow_variable(\"my_catalog\")\n\n        df = catalog.load_pandas(\"foo\", where=\"event_date &gt;= '2024-01-01'\")\n        out = transform(df)     # Function should be pure, only IO happens in the pipeline with the catalog\n        catalog.save_pandas(\"foo_processed\", out)\n\n    get_input_data()\n    ds_pipeline()\n\n\nif __name__ == \"__main__\":\n    dag.test()\n</code></pre> <p>After the run, check <code>/tmp/foo_processed.csv</code> for the transformed result.</p>"},{"location":"examples/standalone/","title":"Smallcat standalone","text":"<p>Use Smallcat without any dependencies.</p>"},{"location":"examples/standalone/#setup","title":"Setup","text":""},{"location":"examples/standalone/#install-smallcat","title":"Install smallcat","text":"<pre><code>pip install smallcat\n</code></pre> <p>or</p> <pre><code>uv add smallcat\n</code></pre>"},{"location":"examples/standalone/#create-an-example-catalog","title":"Create an example catalog","text":"<p>Create a file named <code>catalog.yaml</code> with the following contents.</p> <pre><code># yaml-language-server: $schema=https://raw.githubusercontent.com/DeepKernelLabs/smallcat/refs/heads/main/schemas/catalog.schema.json\nentries:\n  foo:\n    file_format: csv\n    connection:\n      conn_type: fs\n      extra:\n        base_path: /tmp/\n    location: foo.csv\n    save_options:\n      header: true\n    load_options:\n      header: true\n</code></pre> <p>The <code># yaml-language-server</code> line gives you intellisense in VS code, so it will validate and auto-complete your catalog.</p>"},{"location":"examples/standalone/#usage","title":"Usage","text":""},{"location":"examples/standalone/#create-mainpy","title":"Create main.py","text":"<p>Create a python file with the following contents:</p> <pre><code>import pandas as pd\nfrom smallcat.catalog import Catalog\n\n\ndef get_example_dataset() -&gt; pd.DataFrame:\n    url = \"https://raw.githubusercontent.com/vega/vega-datasets/master/data/seattle-weather.csv\"\n    return pd.read_csv(url, parse_dates=[\"date\"])\n\n\ndef main():\n    print(\"Hello from smallcat-local-example!\")\n\n    catalog = Catalog.from_yaml('catalog.yaml')\n    catalog.save_pandas('foo', df=get_example_dataset())\n\n    df = catalog.load_pandas('foo', where=\"precipitation &gt; 0\")\n    print(df.head())\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/standalone/#run-the-file","title":"Run the file","text":"<pre><code>uv run main.py\n</code></pre> <p>Output:</p> <pre><code>Hello from smallcat-local-example!\n        date  precipitation  temp_max  temp_min  wind  weather\n0 2012-01-01            0.0      12.8       5.0   4.7  drizzle\n1 2012-01-02           10.9      10.6       2.8   4.5     rain\n2 2012-01-03            0.8      11.7       7.2   2.3     rain\n3 2012-01-04           20.3      12.2       5.6   4.7     rain\n4 2012-01-05            1.3       8.9       2.8   6.1     rain\n</code></pre>"}]}